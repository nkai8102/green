{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2ef3d0-c8bc-4511-934e-7205c5825dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "# from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from optuna import integration, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca09f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_csv(\"feat/feat_train_multiclass.csv\")\n",
    "test = pl.read_csv(\"feat/feat_test.csv\")\n",
    "\n",
    "cols_exp = [c for c in test.columns if c != \"idx\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990efad",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc77d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgbm_params(train, cols_exp, col_target):    \n",
    "    params = {\n",
    "        'objective': 'multiclass', \n",
    "        'num_class': 3, \n",
    "        \"metric\": \"multi_logloss\",\n",
    "        # \"force_col_wise\": True, \n",
    "        \"random_seed\": 0, \n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    x = train[cols_exp].to_numpy()\n",
    "    y = train[col_target].to_numpy()\n",
    "\n",
    "    # dataset\n",
    "    train_set = integration.lightgbm.Dataset(x, y)\n",
    "\n",
    "    # tuning with optuna\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=False)\n",
    "    tuner = integration.lightgbm.LightGBMTunerCV(params=params, \n",
    "                                                train_set=train_set, \n",
    "                                                num_boost_round=100, \n",
    "                                                # num_boost_round=5, \n",
    "                                                folds=list(skf.split(x, y)))\n",
    "    logging.set_verbosity(logging.WARNING)\n",
    "    tuner.run()\n",
    "\n",
    "    params_tuned = tuner.best_params\n",
    "    return params_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3c42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(train, cols_exp, col_target, params=None):\n",
    "    \n",
    "    if params is None:\n",
    "        params = {}\n",
    "        \n",
    "    params_add = {\n",
    "        'objective': 'multiclass', \n",
    "        'num_class': 3, \n",
    "        \"n_estimators\": 10000, \n",
    "        \"metric\": \"multi_logloss\",\n",
    "        # \"force_col_wise\": True\n",
    "    }\n",
    "    params |= params_add\n",
    "\n",
    "    x = train[cols_exp].to_numpy()\n",
    "    y = train[col_target].to_numpy()\n",
    "\n",
    "    # # down sampling\n",
    "    # sampler = RandomUnderSampler(random_state=42)\n",
    "    # x, y = sampler.fit_resample(x, y)\n",
    "    \n",
    "    # 層化K-fold\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=False)\n",
    "    y_valid_pred_lst = []\n",
    "    idx_valid_lst = []\n",
    "    clf_lst = []\n",
    "\n",
    "    # cross validation\n",
    "    for fold, (idx_train, idx_valid) in enumerate(skf.split(x, y)):\n",
    "        print(\"fold\", fold)\n",
    "        x_train = x[idx_train, :]\n",
    "        x_valid = x[idx_valid, :]\n",
    "        y_train = y[idx_train]\n",
    "        y_valid = y[idx_valid]\n",
    "\n",
    "        # lightgbm modeling\n",
    "        clf = lgbm.LGBMClassifier(**params)#, verbose=0)\n",
    "        clf.fit(x_train, y_train, \n",
    "                eval_set=[(x_train, y_train), (x_valid, y_valid)],  \n",
    "                callbacks=[\n",
    "                    lgbm.early_stopping(stopping_rounds=50),\n",
    "                    lgbm.log_evaluation(period=10000),\n",
    "                ])\n",
    "\n",
    "        # oof\n",
    "        y_valid_pred = clf.predict_proba(x_valid)\n",
    "        y_valid_pred_lst.append(y_valid_pred)\n",
    "        idx_valid_lst.append(idx_valid)\n",
    "        clf_lst.append(clf)\n",
    "\n",
    "    idx_valid = np.hstack(idx_valid_lst)\n",
    "    # y_valid_pred = np.hstack(y_valid_pred_lst)\n",
    "    y_valid_pred = np.vstack(y_valid_pred_lst)\n",
    "    oof_pred = y_valid_pred[idx_valid]\n",
    "\n",
    "    return clf_lst, oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960ae40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(test, cols_exp, clf_lst):\n",
    "    x_test = test[cols_exp].to_numpy()\n",
    "    y_test_pred_lst = []\n",
    "\n",
    "    for clf in clf_lst:\n",
    "        y_test_pred = clf.predict_proba(x_test)\n",
    "        y_test_pred_lst.append(y_test_pred)\n",
    "\n",
    "    y_test_pred = np.mean(y_test_pred_lst, axis=0)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f08bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-29 15:09:48,583] A new study created in memory with name: no-name-f77ba559-a0f1-430f-9b3a-b0b219063218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_target = health --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.615968: 100%|##########| 7/7 [00:09<00:00,  1.34s/it]\n",
      "num_leaves, val_score: 0.606078: 100%|##########| 20/20 [00:41<00:00,  2.07s/it]\n",
      "bagging, val_score: 0.605851: 100%|##########| 10/10 [00:04<00:00,  2.17it/s]\n",
      "feature_fraction_stage2, val_score: 0.605851: 100%|##########| 3/3 [00:01<00:00,  2.13it/s]\n",
      "regularization_factors, val_score: 0.605851: 100%|##########| 20/20 [00:09<00:00,  2.15it/s]\n",
      "min_child_samples, val_score: 0.605662: 100%|##########| 5/5 [00:02<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[252]\ttraining's multi_logloss: 0.596847\tvalid_1's multi_logloss: 0.604313\n",
      "fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's multi_logloss: 0.601637\tvalid_1's multi_logloss: 0.605568\n",
      "fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttraining's multi_logloss: 0.600463\tvalid_1's multi_logloss: 0.606178\n",
      "fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[375]\ttraining's multi_logloss: 0.595443\tvalid_1's multi_logloss: 0.603172\n",
      "fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's multi_logloss: 0.598468\tvalid_1's multi_logloss: 0.605868\n"
     ]
    }
   ],
   "source": [
    "col_target = \"health\"\n",
    "print(\"col_target =\", col_target, \"-\"*50)\n",
    "\n",
    "# parameter tuning with optuna\n",
    "params_tuned = tune_lgbm_params(train, cols_exp, col_target)\n",
    "\n",
    "# train LGBM model\n",
    "clf_lst, oof_pred = train_lgbm(train, cols_exp, col_target, params_tuned)\n",
    "\n",
    "# predict test with CV ensemble\n",
    "y_test_pred = predict_test(test, cols_exp, clf_lst)\n",
    "\n",
    "# record\n",
    "oof_pred_df = pl.DataFrame(oof_pred, schema=[f\"health_is_{h}\" for h in range(3)])\n",
    "test_pred_df = pl.DataFrame(y_test_pred, schema=[f\"health_is_{h}\" for h in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf1f771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "oof_pred_df.write_csv(\"pred/oof_pred_df_multiclass.csv\")\n",
    "test_pred_df.write_csv(\"pred/test_pred_df_multiclass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c237432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d00ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f5201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3ba5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
