{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2ef3d0-c8bc-4511-934e-7205c5825dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4d8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"feat00\"\n",
    "train = pl.read_csv(f\"feat/feat_train_{feat}.csv\")\n",
    "test = pl.read_csv(f\"feat/feat_test_{feat}.csv\")\n",
    "train_origin = pl.read_csv(\"data/train.csv\").rename({\"\": \"idx\"})\n",
    "\n",
    "# 説明変数のカラム\n",
    "cols_exp = [c for c in test.columns if c != \"idx\"]\n",
    "\n",
    "# カテゴリ特徴量のカラム\n",
    "cols_notcat = ['idx', 'created_at', 'tree_dbh']\n",
    "cols_cat = [c for c in test.columns if not c in cols_notcat] # カテゴリ特徴量\n",
    "\n",
    "for col in cols_cat:\n",
    "    # もし欠損値があればその特徴量のユニーク数(欠損値を除く)で埋める（ordinal encoding前提）\n",
    "    num_null = train[col].n_unique() - 1\n",
    "    train = train.with_columns(train[col].fill_null(num_null))\n",
    "    test = test.with_columns(test[col].fill_null(num_null))\n",
    "\n",
    "# 100以上のユニーク数をもつ特徴量を削除（train/valid split時にordinal encodingの連番の整合性が崩れるため）\n",
    "# ※ pretrain実施しない場合はコメントアウトする\n",
    "cols_exp = [c for c in cols_exp if not c in (\"boro_ct\", \"spc_latin\", \"nta\")]\n",
    "cols_cat = [c for c in cols_cat if not c in (\"boro_ct\", \"spc_latin\", \"nta\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990efad",
   "metadata": {},
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3c42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabnet(train, cols_exp, cols_cat, col_target, params=None):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    params_add = {\"device_name\": \"cpu\", \"seed\": 0}\n",
    "    params |= params_add\n",
    "\n",
    "    x = train[cols_exp].to_numpy()\n",
    "    y = train[col_target].to_numpy()\n",
    "\n",
    "    # cols_expにおけるカテゴリ変数のインデックス\n",
    "    cols_cat_idxs = [i for i, c in enumerate(cols_exp) if c in cols_cat]\n",
    "    cols_num_idxs = [i for i, c in enumerate(cols_exp) if not c in cols_cat]\n",
    "    cols_cat_dims = train.approx_n_unique()[cols_exp].to_numpy().ravel()[cols_cat_idxs]\n",
    "\n",
    "    # K-fold\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "    y_valid_pred_lst = []\n",
    "    idx_valid_lst = []\n",
    "    clf_lst = []\n",
    "\n",
    "    # cross validation\n",
    "    for fold, (idx_train, idx_valid) in enumerate(kf.split(x)):\n",
    "        print(\"fold\", fold)\n",
    "        x_train = x[idx_train, :]\n",
    "        x_valid = x[idx_valid, :]\n",
    "        y_train = y[idx_train]\n",
    "        y_valid = y[idx_valid]\n",
    "        \n",
    "        # normalization\n",
    "        scaler = StandardScaler()\n",
    "        x_train[:, cols_num_idxs] = scaler.fit_transform(x_train[:, cols_num_idxs])\n",
    "        x_valid[:, cols_num_idxs] = scaler.transform(x_valid[:, cols_num_idxs])\n",
    "\n",
    "        # modeling\n",
    "        pretrainer = TabNetPretrainer(**params)\n",
    "        pretrainer.fit(x_train, eval_set=[x_valid])\n",
    "        clf = TabNetClassifier(**params, cat_idxs=cols_cat_idxs, cat_dims=cols_cat_dims)\n",
    "        clf.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=[\"logloss\"], \n",
    "            from_unsupervised=pretrainer\n",
    "        )   \n",
    "        \n",
    "        # oof\n",
    "        y_valid_pred = clf.predict_proba(x_valid)\n",
    "        y_valid_pred_lst.append(y_valid_pred)\n",
    "        idx_valid_lst.append(idx_valid)\n",
    "        clf_lst.append(clf)\n",
    "\n",
    "    idx_valid = np.hstack(idx_valid_lst)\n",
    "    y_valid_pred = np.vstack(y_valid_pred_lst)\n",
    "    oof_pred = y_valid_pred[np.argsort(idx_valid)]\n",
    "\n",
    "    return clf_lst, oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960ae40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(x_test, clf_lst):\n",
    "    y_test_pred_lst = []\n",
    "\n",
    "    for clf in clf_lst:\n",
    "        y_test_pred = clf.predict_proba(x_test)\n",
    "        y_test_pred_lst.append(y_test_pred)\n",
    "\n",
    "    y_test_pred = np.mean(y_test_pred_lst, axis=0)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4adc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 28.8181 | val_0_unsup_loss_numpy: 918.6209716796875|  0:00:00s\n",
      "epoch 1  | loss: 3.33351 | val_0_unsup_loss_numpy: 71.92803955078125|  0:00:01s\n",
      "epoch 2  | loss: 1.72215 | val_0_unsup_loss_numpy: 15.211400032043457|  0:00:02s\n",
      "epoch 3  | loss: 1.3846  | val_0_unsup_loss_numpy: 7.321239948272705|  0:00:03s\n",
      "epoch 4  | loss: 1.22386 | val_0_unsup_loss_numpy: 1.7450499534606934|  0:00:04s\n",
      "epoch 5  | loss: 1.19924 | val_0_unsup_loss_numpy: 2.343100070953369|  0:00:05s\n",
      "epoch 6  | loss: 1.18146 | val_0_unsup_loss_numpy: 2.298029899597168|  0:00:07s\n",
      "epoch 7  | loss: 1.12498 | val_0_unsup_loss_numpy: 1.262120008468628|  0:00:07s\n",
      "epoch 8  | loss: 1.03204 | val_0_unsup_loss_numpy: 2.7106399536132812|  0:00:08s\n",
      "epoch 9  | loss: 1.22404 | val_0_unsup_loss_numpy: 2.265470027923584|  0:00:09s\n",
      "epoch 10 | loss: 1.08398 | val_0_unsup_loss_numpy: 1.0026099681854248|  0:00:10s\n",
      "epoch 11 | loss: 1.08066 | val_0_unsup_loss_numpy: 3.1536900997161865|  0:00:11s\n",
      "epoch 12 | loss: 1.04316 | val_0_unsup_loss_numpy: 1.1666500568389893|  0:00:12s\n",
      "epoch 13 | loss: 1.01765 | val_0_unsup_loss_numpy: 0.9563500285148621|  0:00:13s\n",
      "epoch 14 | loss: 0.99015 | val_0_unsup_loss_numpy: 1.0171699523925781|  0:00:14s\n",
      "epoch 15 | loss: 1.09371 | val_0_unsup_loss_numpy: 1.2947499752044678|  0:00:15s\n",
      "epoch 16 | loss: 1.30881 | val_0_unsup_loss_numpy: 1.2061500549316406|  0:00:16s\n",
      "epoch 17 | loss: 1.12656 | val_0_unsup_loss_numpy: 1.4598100185394287|  0:00:17s\n",
      "epoch 18 | loss: 0.96144 | val_0_unsup_loss_numpy: 0.9874699711799622|  0:00:17s\n",
      "epoch 19 | loss: 0.99878 | val_0_unsup_loss_numpy: 1.313670039176941|  0:00:18s\n",
      "epoch 20 | loss: 1.02798 | val_0_unsup_loss_numpy: 1.3701200485229492|  0:00:19s\n",
      "epoch 21 | loss: 0.999   | val_0_unsup_loss_numpy: 1.3081300258636475|  0:00:20s\n",
      "epoch 22 | loss: 1.09787 | val_0_unsup_loss_numpy: 2.859689950942993|  0:00:21s\n",
      "epoch 23 | loss: 1.09921 | val_0_unsup_loss_numpy: 0.8979799747467041|  0:00:22s\n",
      "epoch 24 | loss: 1.06109 | val_0_unsup_loss_numpy: 1.4142500162124634|  0:00:23s\n",
      "epoch 25 | loss: 0.96833 | val_0_unsup_loss_numpy: 0.940280020236969|  0:00:24s\n",
      "epoch 26 | loss: 0.94633 | val_0_unsup_loss_numpy: 0.9231399893760681|  0:00:25s\n",
      "epoch 27 | loss: 0.98275 | val_0_unsup_loss_numpy: 0.8759300112724304|  0:00:25s\n",
      "epoch 28 | loss: 0.95067 | val_0_unsup_loss_numpy: 0.8579999804496765|  0:00:26s\n",
      "epoch 29 | loss: 1.02867 | val_0_unsup_loss_numpy: 0.8624600172042847|  0:00:27s\n",
      "epoch 30 | loss: 1.00614 | val_0_unsup_loss_numpy: 0.8438900113105774|  0:00:28s\n",
      "epoch 31 | loss: 0.97031 | val_0_unsup_loss_numpy: 0.9803599715232849|  0:00:29s\n",
      "epoch 32 | loss: 1.07516 | val_0_unsup_loss_numpy: 0.955780029296875|  0:00:30s\n",
      "epoch 33 | loss: 0.97796 | val_0_unsup_loss_numpy: 2.188380002975464|  0:00:31s\n",
      "epoch 34 | loss: 0.92488 | val_0_unsup_loss_numpy: 0.8478599786758423|  0:00:32s\n",
      "epoch 35 | loss: 0.93937 | val_0_unsup_loss_numpy: 1.0513700246810913|  0:00:33s\n",
      "epoch 36 | loss: 0.8879  | val_0_unsup_loss_numpy: 0.9396200180053711|  0:00:34s\n",
      "epoch 37 | loss: 0.90075 | val_0_unsup_loss_numpy: 1.1601200103759766|  0:00:34s\n",
      "epoch 38 | loss: 0.99101 | val_0_unsup_loss_numpy: 1.1012699604034424|  0:00:35s\n",
      "epoch 39 | loss: 0.9006  | val_0_unsup_loss_numpy: 0.8539100289344788|  0:00:36s\n",
      "epoch 40 | loss: 0.90375 | val_0_unsup_loss_numpy: 0.8776400089263916|  0:00:37s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_unsup_loss_numpy = 0.8438900113105774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.74198 | train_logloss: 0.89261 | valid_logloss: 0.8958  |  0:00:01s\n",
      "epoch 1  | loss: 0.6266  | train_logloss: 0.75247 | valid_logloss: 0.76859 |  0:00:02s\n",
      "epoch 2  | loss: 0.6147  | train_logloss: 0.68914 | valid_logloss: 0.70327 |  0:00:03s\n",
      "epoch 3  | loss: 0.61167 | train_logloss: 0.65762 | valid_logloss: 0.67227 |  0:00:04s\n",
      "epoch 4  | loss: 0.61163 | train_logloss: 0.64534 | valid_logloss: 0.65692 |  0:00:05s\n",
      "epoch 5  | loss: 0.61257 | train_logloss: 0.63846 | valid_logloss: 0.6497  |  0:00:06s\n",
      "epoch 6  | loss: 0.61259 | train_logloss: 0.62375 | valid_logloss: 0.63746 |  0:00:07s\n",
      "epoch 7  | loss: 0.6118  | train_logloss: 0.60888 | valid_logloss: 0.62084 |  0:00:08s\n",
      "epoch 8  | loss: 0.6124  | train_logloss: 0.60846 | valid_logloss: 0.61898 |  0:00:09s\n",
      "epoch 9  | loss: 0.61026 | train_logloss: 0.60796 | valid_logloss: 0.61975 |  0:00:10s\n",
      "epoch 10 | loss: 0.61045 | train_logloss: 0.60714 | valid_logloss: 0.61905 |  0:00:11s\n",
      "epoch 11 | loss: 0.60769 | train_logloss: 0.60898 | valid_logloss: 0.62058 |  0:00:12s\n",
      "epoch 12 | loss: 0.60893 | train_logloss: 0.60745 | valid_logloss: 0.61979 |  0:00:13s\n",
      "epoch 13 | loss: 0.60867 | train_logloss: 0.60705 | valid_logloss: 0.61907 |  0:00:14s\n",
      "epoch 14 | loss: 0.60815 | train_logloss: 0.60637 | valid_logloss: 0.6197  |  0:00:15s\n",
      "epoch 15 | loss: 0.60829 | train_logloss: 0.60645 | valid_logloss: 0.61998 |  0:00:17s\n",
      "epoch 16 | loss: 0.6101  | train_logloss: 0.60727 | valid_logloss: 0.62313 |  0:00:18s\n",
      "epoch 17 | loss: 0.60786 | train_logloss: 0.60583 | valid_logloss: 0.62    |  0:00:19s\n",
      "epoch 18 | loss: 0.60678 | train_logloss: 0.60547 | valid_logloss: 0.61859 |  0:00:20s\n",
      "epoch 19 | loss: 0.60723 | train_logloss: 0.60564 | valid_logloss: 0.61847 |  0:00:21s\n",
      "epoch 20 | loss: 0.60758 | train_logloss: 0.60603 | valid_logloss: 0.61922 |  0:00:22s\n",
      "epoch 21 | loss: 0.60914 | train_logloss: 0.61093 | valid_logloss: 0.62286 |  0:00:23s\n",
      "epoch 22 | loss: 0.60769 | train_logloss: 0.6051  | valid_logloss: 0.61863 |  0:00:24s\n",
      "epoch 23 | loss: 0.60664 | train_logloss: 0.60489 | valid_logloss: 0.61735 |  0:00:25s\n",
      "epoch 24 | loss: 0.60737 | train_logloss: 0.60565 | valid_logloss: 0.61663 |  0:00:26s\n",
      "epoch 25 | loss: 0.60709 | train_logloss: 0.60507 | valid_logloss: 0.617   |  0:00:27s\n",
      "epoch 26 | loss: 0.60836 | train_logloss: 0.60513 | valid_logloss: 0.61737 |  0:00:28s\n",
      "epoch 27 | loss: 0.60554 | train_logloss: 0.6048  | valid_logloss: 0.61779 |  0:00:29s\n",
      "epoch 28 | loss: 0.60345 | train_logloss: 0.60643 | valid_logloss: 0.61894 |  0:00:30s\n",
      "epoch 29 | loss: 0.6083  | train_logloss: 0.60419 | valid_logloss: 0.61621 |  0:00:31s\n",
      "epoch 30 | loss: 0.6053  | train_logloss: 0.60418 | valid_logloss: 0.61812 |  0:00:32s\n",
      "epoch 31 | loss: 0.60562 | train_logloss: 0.60464 | valid_logloss: 0.61847 |  0:00:33s\n",
      "epoch 32 | loss: 0.60658 | train_logloss: 0.60564 | valid_logloss: 0.62237 |  0:00:35s\n",
      "epoch 33 | loss: 0.60747 | train_logloss: 0.60426 | valid_logloss: 0.61974 |  0:00:36s\n",
      "epoch 34 | loss: 0.60647 | train_logloss: 0.60385 | valid_logloss: 0.61978 |  0:00:37s\n",
      "epoch 35 | loss: 0.60935 | train_logloss: 0.60297 | valid_logloss: 0.62005 |  0:00:38s\n",
      "epoch 36 | loss: 0.60234 | train_logloss: 0.60517 | valid_logloss: 0.6193  |  0:00:39s\n",
      "epoch 37 | loss: 0.60591 | train_logloss: 0.60378 | valid_logloss: 0.6188  |  0:00:40s\n",
      "epoch 38 | loss: 0.60508 | train_logloss: 0.60411 | valid_logloss: 0.62016 |  0:00:41s\n",
      "epoch 39 | loss: 0.60334 | train_logloss: 0.60364 | valid_logloss: 0.61947 |  0:00:42s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_valid_logloss = 0.61621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 31.16613| val_0_unsup_loss_numpy: 13.487449645996094|  0:00:00s\n",
      "epoch 1  | loss: 3.44688 | val_0_unsup_loss_numpy: 6.697360038757324|  0:00:01s\n",
      "epoch 2  | loss: 1.89554 | val_0_unsup_loss_numpy: 4.012030124664307|  0:00:02s\n",
      "epoch 3  | loss: 1.43937 | val_0_unsup_loss_numpy: 1.5916199684143066|  0:00:03s\n",
      "epoch 4  | loss: 1.25375 | val_0_unsup_loss_numpy: 1.3181899785995483|  0:00:04s\n",
      "epoch 5  | loss: 1.18857 | val_0_unsup_loss_numpy: 1.3203599452972412|  0:00:05s\n",
      "epoch 6  | loss: 1.12348 | val_0_unsup_loss_numpy: 1.1567200422286987|  0:00:06s\n",
      "epoch 7  | loss: 1.06686 | val_0_unsup_loss_numpy: 1.0848100185394287|  0:00:07s\n",
      "epoch 8  | loss: 1.05477 | val_0_unsup_loss_numpy: 1.1174800395965576|  0:00:08s\n",
      "epoch 9  | loss: 1.03106 | val_0_unsup_loss_numpy: 1.2991700172424316|  0:00:08s\n",
      "epoch 10 | loss: 1.03883 | val_0_unsup_loss_numpy: 1.2236100435256958|  0:00:09s\n",
      "epoch 11 | loss: 1.01266 | val_0_unsup_loss_numpy: 1.1615400314331055|  0:00:10s\n",
      "epoch 12 | loss: 1.02552 | val_0_unsup_loss_numpy: 1.4857499599456787|  0:00:11s\n",
      "epoch 13 | loss: 1.17932 | val_0_unsup_loss_numpy: 1.4683200120925903|  0:00:12s\n",
      "epoch 14 | loss: 1.0221  | val_0_unsup_loss_numpy: 1.894029974937439|  0:00:13s\n",
      "epoch 15 | loss: 0.97552 | val_0_unsup_loss_numpy: 1.1214200258255005|  0:00:14s\n",
      "epoch 16 | loss: 0.98117 | val_0_unsup_loss_numpy: 1.2497999668121338|  0:00:15s\n",
      "epoch 17 | loss: 0.9722  | val_0_unsup_loss_numpy: 1.0249300003051758|  0:00:15s\n",
      "epoch 18 | loss: 0.98633 | val_0_unsup_loss_numpy: 1.0111299753189087|  0:00:16s\n",
      "epoch 19 | loss: 0.94109 | val_0_unsup_loss_numpy: 0.9996299743652344|  0:00:17s\n",
      "epoch 20 | loss: 0.9366  | val_0_unsup_loss_numpy: 2.2211201190948486|  0:00:18s\n",
      "epoch 21 | loss: 1.25684 | val_0_unsup_loss_numpy: 2.5854599475860596|  0:00:19s\n",
      "epoch 22 | loss: 1.29584 | val_0_unsup_loss_numpy: 2.1638998985290527|  0:00:20s\n",
      "epoch 23 | loss: 1.04367 | val_0_unsup_loss_numpy: 0.953760027885437|  0:00:21s\n",
      "epoch 24 | loss: 0.91515 | val_0_unsup_loss_numpy: 0.9296799898147583|  0:00:22s\n",
      "epoch 25 | loss: 0.95658 | val_0_unsup_loss_numpy: 0.9071400165557861|  0:00:22s\n",
      "epoch 26 | loss: 0.94839 | val_0_unsup_loss_numpy: 0.9756900072097778|  0:00:23s\n",
      "epoch 27 | loss: 0.93027 | val_0_unsup_loss_numpy: 0.9383699893951416|  0:00:24s\n",
      "epoch 28 | loss: 0.91524 | val_0_unsup_loss_numpy: 0.9341700077056885|  0:00:25s\n",
      "epoch 29 | loss: 0.8931  | val_0_unsup_loss_numpy: 0.9251800179481506|  0:00:26s\n",
      "epoch 30 | loss: 0.90974 | val_0_unsup_loss_numpy: 0.9005900025367737|  0:00:27s\n",
      "epoch 31 | loss: 0.88992 | val_0_unsup_loss_numpy: 1.066349983215332|  0:00:28s\n",
      "epoch 32 | loss: 0.8721  | val_0_unsup_loss_numpy: 1.0071500539779663|  0:00:29s\n",
      "epoch 33 | loss: 0.94063 | val_0_unsup_loss_numpy: 1.014140009880066|  0:00:30s\n",
      "epoch 34 | loss: 0.91689 | val_0_unsup_loss_numpy: 0.8920800089836121|  0:00:30s\n",
      "epoch 35 | loss: 0.88691 | val_0_unsup_loss_numpy: 1.0783100128173828|  0:00:31s\n",
      "epoch 36 | loss: 0.92413 | val_0_unsup_loss_numpy: 1.2191499471664429|  0:00:32s\n",
      "epoch 37 | loss: 0.89626 | val_0_unsup_loss_numpy: 0.8726900219917297|  0:00:33s\n",
      "epoch 38 | loss: 0.87953 | val_0_unsup_loss_numpy: 0.9342700242996216|  0:00:34s\n",
      "epoch 39 | loss: 0.91168 | val_0_unsup_loss_numpy: 1.119320034980774|  0:00:35s\n",
      "epoch 40 | loss: 0.89649 | val_0_unsup_loss_numpy: 0.8925999999046326|  0:00:36s\n",
      "epoch 41 | loss: 0.93544 | val_0_unsup_loss_numpy: 1.015720009803772|  0:00:37s\n",
      "epoch 42 | loss: 0.90878 | val_0_unsup_loss_numpy: 1.1699299812316895|  0:00:37s\n",
      "epoch 43 | loss: 0.96474 | val_0_unsup_loss_numpy: 1.4453599452972412|  0:00:38s\n",
      "epoch 44 | loss: 0.92949 | val_0_unsup_loss_numpy: 0.9651299715042114|  0:00:39s\n",
      "epoch 45 | loss: 0.93163 | val_0_unsup_loss_numpy: 0.9371299743652344|  0:00:40s\n",
      "epoch 46 | loss: 0.90043 | val_0_unsup_loss_numpy: 0.8479200005531311|  0:00:41s\n",
      "epoch 47 | loss: 0.88708 | val_0_unsup_loss_numpy: 0.8519399762153625|  0:00:42s\n",
      "epoch 48 | loss: 0.874   | val_0_unsup_loss_numpy: 0.8743600249290466|  0:00:43s\n",
      "epoch 49 | loss: 0.88395 | val_0_unsup_loss_numpy: 0.8470100164413452|  0:00:44s\n",
      "epoch 50 | loss: 0.87809 | val_0_unsup_loss_numpy: 0.9392799735069275|  0:00:45s\n",
      "epoch 51 | loss: 0.8785  | val_0_unsup_loss_numpy: 0.886900007724762|  0:00:46s\n",
      "epoch 52 | loss: 0.91595 | val_0_unsup_loss_numpy: 0.8695300221443176|  0:00:47s\n",
      "epoch 53 | loss: 0.87175 | val_0_unsup_loss_numpy: 0.9472000002861023|  0:00:48s\n",
      "epoch 54 | loss: 0.92543 | val_0_unsup_loss_numpy: 1.0128400325775146|  0:00:49s\n",
      "epoch 55 | loss: 1.0471  | val_0_unsup_loss_numpy: 1.804110050201416|  0:00:49s\n",
      "epoch 56 | loss: 1.10946 | val_0_unsup_loss_numpy: 1.643839955329895|  0:00:50s\n",
      "epoch 57 | loss: 1.22693 | val_0_unsup_loss_numpy: 0.9431399703025818|  0:00:51s\n",
      "epoch 58 | loss: 0.97954 | val_0_unsup_loss_numpy: 0.83433997631073|  0:00:52s\n",
      "epoch 59 | loss: 0.9835  | val_0_unsup_loss_numpy: 0.8250799775123596|  0:00:53s\n",
      "epoch 60 | loss: 0.92135 | val_0_unsup_loss_numpy: 0.9144899845123291|  0:00:54s\n",
      "epoch 61 | loss: 0.93615 | val_0_unsup_loss_numpy: 0.875   |  0:00:55s\n",
      "epoch 62 | loss: 0.89569 | val_0_unsup_loss_numpy: 0.9064499735832214|  0:00:56s\n",
      "epoch 63 | loss: 0.89281 | val_0_unsup_loss_numpy: 1.5008599758148193|  0:00:57s\n",
      "epoch 64 | loss: 0.8801  | val_0_unsup_loss_numpy: 1.5906000137329102|  0:00:58s\n",
      "epoch 65 | loss: 0.94616 | val_0_unsup_loss_numpy: 0.8141000270843506|  0:00:59s\n",
      "epoch 66 | loss: 0.87871 | val_0_unsup_loss_numpy: 0.8321999907493591|  0:01:00s\n",
      "epoch 67 | loss: 0.85127 | val_0_unsup_loss_numpy: 0.8960199952125549|  0:01:01s\n",
      "epoch 68 | loss: 0.8606  | val_0_unsup_loss_numpy: 0.8579300045967102|  0:01:02s\n",
      "epoch 69 | loss: 0.86121 | val_0_unsup_loss_numpy: 1.343309998512268|  0:01:03s\n",
      "epoch 70 | loss: 1.12024 | val_0_unsup_loss_numpy: 1.3186999559402466|  0:01:04s\n",
      "epoch 71 | loss: 0.91175 | val_0_unsup_loss_numpy: 0.9453999996185303|  0:01:05s\n",
      "epoch 72 | loss: 0.98259 | val_0_unsup_loss_numpy: 1.567199945449829|  0:01:06s\n",
      "epoch 73 | loss: 0.95031 | val_0_unsup_loss_numpy: 1.6269999742507935|  0:01:07s\n",
      "epoch 74 | loss: 0.93279 | val_0_unsup_loss_numpy: 0.8576300144195557|  0:01:07s\n",
      "epoch 75 | loss: 0.92313 | val_0_unsup_loss_numpy: 0.8419700264930725|  0:01:08s\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 65 and best_val_0_unsup_loss_numpy = 0.8141000270843506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6875  | train_logloss: 0.67518 | valid_logloss: 0.67746 |  0:00:01s\n",
      "epoch 1  | loss: 0.62248 | train_logloss: 0.63321 | valid_logloss: 0.64365 |  0:00:02s\n",
      "epoch 2  | loss: 0.61886 | train_logloss: 0.62167 | valid_logloss: 0.63158 |  0:00:03s\n",
      "epoch 3  | loss: 0.61476 | train_logloss: 0.61495 | valid_logloss: 0.62449 |  0:00:04s\n",
      "epoch 4  | loss: 0.61046 | train_logloss: 0.61515 | valid_logloss: 0.62565 |  0:00:05s\n",
      "epoch 5  | loss: 0.60681 | train_logloss: 0.60999 | valid_logloss: 0.61884 |  0:00:06s\n",
      "epoch 6  | loss: 0.61036 | train_logloss: 0.60823 | valid_logloss: 0.61683 |  0:00:07s\n",
      "epoch 7  | loss: 0.60674 | train_logloss: 0.60791 | valid_logloss: 0.61657 |  0:00:09s\n",
      "epoch 8  | loss: 0.60942 | train_logloss: 0.60723 | valid_logloss: 0.61532 |  0:00:10s\n",
      "epoch 9  | loss: 0.6068  | train_logloss: 0.60714 | valid_logloss: 0.61636 |  0:00:11s\n",
      "epoch 10 | loss: 0.61044 | train_logloss: 0.60807 | valid_logloss: 0.61513 |  0:00:12s\n",
      "epoch 11 | loss: 0.60643 | train_logloss: 0.60829 | valid_logloss: 0.61671 |  0:00:13s\n",
      "epoch 12 | loss: 0.60641 | train_logloss: 0.60603 | valid_logloss: 0.61458 |  0:00:14s\n",
      "epoch 13 | loss: 0.61043 | train_logloss: 0.60636 | valid_logloss: 0.61606 |  0:00:15s\n",
      "epoch 14 | loss: 0.60992 | train_logloss: 0.60734 | valid_logloss: 0.61624 |  0:00:16s\n",
      "epoch 15 | loss: 0.60684 | train_logloss: 0.60711 | valid_logloss: 0.6166  |  0:00:18s\n",
      "epoch 16 | loss: 0.61032 | train_logloss: 0.60671 | valid_logloss: 0.61568 |  0:00:19s\n",
      "epoch 17 | loss: 0.61043 | train_logloss: 0.60555 | valid_logloss: 0.61281 |  0:00:21s\n",
      "epoch 18 | loss: 0.60951 | train_logloss: 0.60601 | valid_logloss: 0.61391 |  0:00:25s\n",
      "epoch 19 | loss: 0.60673 | train_logloss: 0.60495 | valid_logloss: 0.61553 |  0:00:26s\n",
      "epoch 20 | loss: 0.60965 | train_logloss: 0.60503 | valid_logloss: 0.61473 |  0:00:27s\n",
      "epoch 21 | loss: 0.60843 | train_logloss: 0.6044  | valid_logloss: 0.61458 |  0:00:28s\n",
      "epoch 22 | loss: 0.60456 | train_logloss: 0.60514 | valid_logloss: 0.6146  |  0:00:29s\n",
      "epoch 23 | loss: 0.60744 | train_logloss: 0.60522 | valid_logloss: 0.61633 |  0:00:31s\n",
      "epoch 24 | loss: 0.60536 | train_logloss: 0.60438 | valid_logloss: 0.61523 |  0:00:32s\n",
      "epoch 25 | loss: 0.60839 | train_logloss: 0.60688 | valid_logloss: 0.61542 |  0:00:33s\n",
      "epoch 26 | loss: 0.60825 | train_logloss: 0.60891 | valid_logloss: 0.62188 |  0:00:34s\n",
      "epoch 27 | loss: 0.60593 | train_logloss: 0.60589 | valid_logloss: 0.61768 |  0:00:35s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_valid_logloss = 0.61281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 33.50326| val_0_unsup_loss_numpy: 77.64251708984375|  0:00:00s\n",
      "epoch 1  | loss: 4.38511 | val_0_unsup_loss_numpy: 6.991390228271484|  0:00:01s\n",
      "epoch 2  | loss: 1.8272  | val_0_unsup_loss_numpy: 110.569091796875|  0:00:02s\n",
      "epoch 3  | loss: 1.36532 | val_0_unsup_loss_numpy: 5.118649959564209|  0:00:03s\n",
      "epoch 4  | loss: 1.16673 | val_0_unsup_loss_numpy: 3.9049999713897705|  0:00:04s\n",
      "epoch 5  | loss: 1.20438 | val_0_unsup_loss_numpy: 1.3443700075149536|  0:00:05s\n",
      "epoch 6  | loss: 1.15774 | val_0_unsup_loss_numpy: 1.1534700393676758|  0:00:06s\n",
      "epoch 7  | loss: 1.08048 | val_0_unsup_loss_numpy: 1.1506799459457397|  0:00:07s\n",
      "epoch 8  | loss: 1.00594 | val_0_unsup_loss_numpy: 1.1089199781417847|  0:00:08s\n",
      "epoch 9  | loss: 1.01416 | val_0_unsup_loss_numpy: 1.2882399559020996|  0:00:09s\n",
      "epoch 10 | loss: 1.37113 | val_0_unsup_loss_numpy: 9.371789932250977|  0:00:10s\n",
      "epoch 11 | loss: 1.10733 | val_0_unsup_loss_numpy: 1.0227199792861938|  0:00:11s\n",
      "epoch 12 | loss: 1.00037 | val_0_unsup_loss_numpy: 1.068850040435791|  0:00:12s\n",
      "epoch 13 | loss: 1.06829 | val_0_unsup_loss_numpy: 2.3954999446868896|  0:00:13s\n",
      "epoch 14 | loss: 1.0955  | val_0_unsup_loss_numpy: 5.274209976196289|  0:00:14s\n",
      "epoch 15 | loss: 1.17724 | val_0_unsup_loss_numpy: 1.0304399728775024|  0:00:14s\n",
      "epoch 16 | loss: 0.97809 | val_0_unsup_loss_numpy: 1.0145000219345093|  0:00:15s\n",
      "epoch 17 | loss: 0.97274 | val_0_unsup_loss_numpy: 1.0276600122451782|  0:00:16s\n",
      "epoch 18 | loss: 1.45708 | val_0_unsup_loss_numpy: 2.7649900913238525|  0:00:17s\n",
      "epoch 19 | loss: 0.98431 | val_0_unsup_loss_numpy: 0.9886199831962585|  0:00:18s\n",
      "epoch 20 | loss: 0.98805 | val_0_unsup_loss_numpy: 1.2296199798583984|  0:00:19s\n",
      "epoch 21 | loss: 0.98196 | val_0_unsup_loss_numpy: 0.9698799848556519|  0:00:20s\n",
      "epoch 22 | loss: 1.12535 | val_0_unsup_loss_numpy: 1.420799970626831|  0:00:21s\n",
      "epoch 23 | loss: 1.04875 | val_0_unsup_loss_numpy: 1.511780023574829|  0:00:22s\n",
      "epoch 24 | loss: 1.06324 | val_0_unsup_loss_numpy: 1.7100600004196167|  0:00:23s\n",
      "epoch 25 | loss: 1.00642 | val_0_unsup_loss_numpy: 1.9848500490188599|  0:00:24s\n",
      "epoch 26 | loss: 1.22317 | val_0_unsup_loss_numpy: 1.53997004032135|  0:00:25s\n",
      "epoch 27 | loss: 1.00657 | val_0_unsup_loss_numpy: 0.9288600087165833|  0:00:26s\n",
      "epoch 28 | loss: 0.99945 | val_0_unsup_loss_numpy: 0.9041699767112732|  0:00:26s\n",
      "epoch 29 | loss: 1.1215  | val_0_unsup_loss_numpy: 3.9649500846862793|  0:00:27s\n",
      "epoch 30 | loss: 1.218   | val_0_unsup_loss_numpy: 1.123110055923462|  0:00:28s\n",
      "epoch 31 | loss: 0.931   | val_0_unsup_loss_numpy: 0.9253799915313721|  0:00:29s\n",
      "epoch 32 | loss: 1.01995 | val_0_unsup_loss_numpy: 0.887719988822937|  0:00:30s\n",
      "epoch 33 | loss: 0.94003 | val_0_unsup_loss_numpy: 0.9845499992370605|  0:00:31s\n",
      "epoch 34 | loss: 0.95758 | val_0_unsup_loss_numpy: 1.5181299448013306|  0:00:32s\n",
      "epoch 35 | loss: 1.00247 | val_0_unsup_loss_numpy: 0.9965699911117554|  0:00:33s\n",
      "epoch 36 | loss: 1.03771 | val_0_unsup_loss_numpy: 0.9795200228691101|  0:00:34s\n",
      "epoch 37 | loss: 0.91301 | val_0_unsup_loss_numpy: 0.8713899850845337|  0:00:35s\n",
      "epoch 38 | loss: 0.89052 | val_0_unsup_loss_numpy: 0.8289700150489807|  0:00:36s\n",
      "epoch 39 | loss: 0.94746 | val_0_unsup_loss_numpy: 1.1606099605560303|  0:00:37s\n",
      "epoch 40 | loss: 1.01956 | val_0_unsup_loss_numpy: 0.8970299959182739|  0:00:42s\n",
      "epoch 41 | loss: 0.91859 | val_0_unsup_loss_numpy: 0.8622099757194519|  0:00:47s\n",
      "epoch 42 | loss: 0.86915 | val_0_unsup_loss_numpy: 0.8212299942970276|  0:00:52s\n",
      "epoch 43 | loss: 0.88461 | val_0_unsup_loss_numpy: 0.8405699729919434|  0:00:56s\n",
      "epoch 44 | loss: 0.89916 | val_0_unsup_loss_numpy: 0.9443399906158447|  0:01:01s\n",
      "epoch 45 | loss: 0.89125 | val_0_unsup_loss_numpy: 0.822130024433136|  0:01:04s\n",
      "epoch 46 | loss: 0.87782 | val_0_unsup_loss_numpy: 0.8053299784660339|  0:01:07s\n",
      "epoch 47 | loss: 0.94757 | val_0_unsup_loss_numpy: 0.8777700066566467|  0:01:10s\n",
      "epoch 48 | loss: 0.87891 | val_0_unsup_loss_numpy: 0.8784099817276001|  0:01:13s\n",
      "epoch 49 | loss: 0.86025 | val_0_unsup_loss_numpy: 0.797819972038269|  0:01:16s\n",
      "epoch 50 | loss: 0.85198 | val_0_unsup_loss_numpy: 0.9894999861717224|  0:01:18s\n",
      "epoch 51 | loss: 1.01328 | val_0_unsup_loss_numpy: 3.0213100910186768|  0:01:21s\n",
      "epoch 52 | loss: 1.11479 | val_0_unsup_loss_numpy: 0.8687300086021423|  0:01:24s\n",
      "epoch 53 | loss: 0.93871 | val_0_unsup_loss_numpy: 0.8046200275421143|  0:01:26s\n",
      "epoch 54 | loss: 0.86094 | val_0_unsup_loss_numpy: 1.080530047416687|  0:01:29s\n",
      "epoch 55 | loss: 0.89792 | val_0_unsup_loss_numpy: 0.7605100274085999|  0:01:32s\n",
      "epoch 56 | loss: 0.83967 | val_0_unsup_loss_numpy: 0.7955800294876099|  0:01:35s\n",
      "epoch 57 | loss: 0.86419 | val_0_unsup_loss_numpy: 0.7881500124931335|  0:01:38s\n",
      "epoch 58 | loss: 0.88214 | val_0_unsup_loss_numpy: 0.9738699793815613|  0:01:41s\n",
      "epoch 59 | loss: 0.87652 | val_0_unsup_loss_numpy: 0.7950900197029114|  0:01:43s\n",
      "epoch 60 | loss: 0.88472 | val_0_unsup_loss_numpy: 0.7945899963378906|  0:01:46s\n",
      "epoch 61 | loss: 0.85658 | val_0_unsup_loss_numpy: 0.7812100052833557|  0:01:49s\n",
      "epoch 62 | loss: 0.89554 | val_0_unsup_loss_numpy: 0.7632499933242798|  0:01:51s\n",
      "epoch 63 | loss: 0.85021 | val_0_unsup_loss_numpy: 0.8902699947357178|  0:01:56s\n",
      "epoch 64 | loss: 0.85802 | val_0_unsup_loss_numpy: 0.7649000287055969|  0:02:03s\n",
      "epoch 65 | loss: 0.856   | val_0_unsup_loss_numpy: 0.7785599827766418|  0:02:07s\n",
      "\n",
      "Early stopping occurred at epoch 65 with best_epoch = 55 and best_val_0_unsup_loss_numpy = 0.7605100274085999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71659 | train_logloss: 0.68033 | valid_logloss: 0.69825 |  0:00:07s\n",
      "epoch 1  | loss: 0.62568 | train_logloss: 0.62483 | valid_logloss: 0.62634 |  0:00:13s\n",
      "epoch 2  | loss: 0.61738 | train_logloss: 0.61572 | valid_logloss: 0.61875 |  0:00:20s\n",
      "epoch 3  | loss: 0.61249 | train_logloss: 0.61579 | valid_logloss: 0.61703 |  0:00:24s\n",
      "epoch 4  | loss: 0.61265 | train_logloss: 0.61118 | valid_logloss: 0.61414 |  0:00:27s\n",
      "epoch 5  | loss: 0.61146 | train_logloss: 0.61122 | valid_logloss: 0.61612 |  0:00:31s\n",
      "epoch 6  | loss: 0.60922 | train_logloss: 0.60869 | valid_logloss: 0.6135  |  0:00:34s\n",
      "epoch 7  | loss: 0.60761 | train_logloss: 0.60876 | valid_logloss: 0.61541 |  0:00:37s\n",
      "epoch 8  | loss: 0.60924 | train_logloss: 0.60797 | valid_logloss: 0.61147 |  0:00:40s\n",
      "epoch 9  | loss: 0.61042 | train_logloss: 0.6079  | valid_logloss: 0.61107 |  0:00:43s\n",
      "epoch 10 | loss: 0.6086  | train_logloss: 0.60657 | valid_logloss: 0.61256 |  0:00:47s\n",
      "epoch 11 | loss: 0.6085  | train_logloss: 0.60576 | valid_logloss: 0.61255 |  0:00:50s\n",
      "epoch 12 | loss: 0.60696 | train_logloss: 0.60636 | valid_logloss: 0.6122  |  0:00:53s\n",
      "epoch 13 | loss: 0.61042 | train_logloss: 0.60759 | valid_logloss: 0.61348 |  0:00:56s\n",
      "epoch 14 | loss: 0.60948 | train_logloss: 0.60581 | valid_logloss: 0.61238 |  0:01:00s\n",
      "epoch 15 | loss: 0.60801 | train_logloss: 0.6076  | valid_logloss: 0.61312 |  0:01:04s\n",
      "epoch 16 | loss: 0.60656 | train_logloss: 0.61012 | valid_logloss: 0.61306 |  0:01:07s\n",
      "epoch 17 | loss: 0.61029 | train_logloss: 0.6082  | valid_logloss: 0.61128 |  0:01:11s\n",
      "epoch 18 | loss: 0.60764 | train_logloss: 0.60581 | valid_logloss: 0.61082 |  0:01:14s\n",
      "epoch 19 | loss: 0.6075  | train_logloss: 0.60555 | valid_logloss: 0.61246 |  0:01:18s\n",
      "epoch 20 | loss: 0.60977 | train_logloss: 0.60593 | valid_logloss: 0.61222 |  0:01:21s\n",
      "epoch 21 | loss: 0.60707 | train_logloss: 0.60563 | valid_logloss: 0.61083 |  0:01:24s\n",
      "epoch 22 | loss: 0.60685 | train_logloss: 0.60509 | valid_logloss: 0.61125 |  0:01:27s\n",
      "epoch 23 | loss: 0.60649 | train_logloss: 0.60522 | valid_logloss: 0.61136 |  0:01:31s\n",
      "epoch 24 | loss: 0.60841 | train_logloss: 0.60493 | valid_logloss: 0.61007 |  0:01:35s\n",
      "epoch 25 | loss: 0.60777 | train_logloss: 0.60434 | valid_logloss: 0.61134 |  0:01:38s\n",
      "epoch 26 | loss: 0.60554 | train_logloss: 0.60337 | valid_logloss: 0.61255 |  0:01:44s\n",
      "epoch 27 | loss: 0.60585 | train_logloss: 0.6051  | valid_logloss: 0.61225 |  0:01:50s\n",
      "epoch 28 | loss: 0.60588 | train_logloss: 0.60393 | valid_logloss: 0.61092 |  0:01:56s\n",
      "epoch 29 | loss: 0.60448 | train_logloss: 0.60526 | valid_logloss: 0.61401 |  0:02:02s\n",
      "epoch 30 | loss: 0.60408 | train_logloss: 0.6025  | valid_logloss: 0.61166 |  0:02:08s\n",
      "epoch 31 | loss: 0.60638 | train_logloss: 0.60269 | valid_logloss: 0.6111  |  0:02:13s\n",
      "epoch 32 | loss: 0.60492 | train_logloss: 0.6026  | valid_logloss: 0.6126  |  0:02:20s\n",
      "epoch 33 | loss: 0.6054  | train_logloss: 0.60345 | valid_logloss: 0.61206 |  0:02:26s\n",
      "epoch 34 | loss: 0.60879 | train_logloss: 0.60273 | valid_logloss: 0.61332 |  0:02:32s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_valid_logloss = 0.61007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 23.10862| val_0_unsup_loss_numpy: 111.81297302246094|  0:00:05s\n",
      "epoch 1  | loss: 3.57288 | val_0_unsup_loss_numpy: 2.5347800254821777|  0:00:08s\n",
      "epoch 2  | loss: 1.72308 | val_0_unsup_loss_numpy: 2.92438006401062|  0:00:11s\n",
      "epoch 3  | loss: 1.33641 | val_0_unsup_loss_numpy: 1.649899959564209|  0:00:14s\n",
      "epoch 4  | loss: 1.23878 | val_0_unsup_loss_numpy: 1.967919945716858|  0:00:17s\n",
      "epoch 5  | loss: 1.19382 | val_0_unsup_loss_numpy: 1.538100004196167|  0:00:19s\n",
      "epoch 6  | loss: 1.08358 | val_0_unsup_loss_numpy: 2.203579902648926|  0:00:22s\n",
      "epoch 7  | loss: 1.08237 | val_0_unsup_loss_numpy: 3.623990058898926|  0:00:25s\n",
      "epoch 8  | loss: 1.19017 | val_0_unsup_loss_numpy: 2.926690101623535|  0:00:28s\n",
      "epoch 9  | loss: 1.33867 | val_0_unsup_loss_numpy: 1.0580300092697144|  0:00:31s\n",
      "epoch 10 | loss: 1.16182 | val_0_unsup_loss_numpy: 1.158560037612915|  0:00:34s\n",
      "epoch 11 | loss: 1.0244  | val_0_unsup_loss_numpy: 1.731279969215393|  0:00:37s\n",
      "epoch 12 | loss: 1.03484 | val_0_unsup_loss_numpy: 1.0097700357437134|  0:00:39s\n",
      "epoch 13 | loss: 0.98517 | val_0_unsup_loss_numpy: 1.3699300289154053|  0:00:43s\n",
      "epoch 14 | loss: 1.05274 | val_0_unsup_loss_numpy: 1.4923900365829468|  0:00:45s\n",
      "epoch 15 | loss: 0.98159 | val_0_unsup_loss_numpy: 1.116379976272583|  0:00:48s\n",
      "epoch 16 | loss: 1.02678 | val_0_unsup_loss_numpy: 1.1146999597549438|  0:00:51s\n",
      "epoch 17 | loss: 0.99239 | val_0_unsup_loss_numpy: 1.9456700086593628|  0:00:54s\n",
      "epoch 18 | loss: 1.00321 | val_0_unsup_loss_numpy: 1.0566400289535522|  0:00:56s\n",
      "epoch 19 | loss: 1.04592 | val_0_unsup_loss_numpy: 2.2196500301361084|  0:00:59s\n",
      "epoch 20 | loss: 1.54768 | val_0_unsup_loss_numpy: 4.95088005065918|  0:01:02s\n",
      "epoch 21 | loss: 1.00365 | val_0_unsup_loss_numpy: 1.3532299995422363|  0:01:05s\n",
      "epoch 22 | loss: 1.00979 | val_0_unsup_loss_numpy: 1.2694799900054932|  0:01:08s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_unsup_loss_numpy = 1.0097700357437134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71304 | train_logloss: 0.72079 | valid_logloss: 0.70695 |  0:00:03s\n",
      "epoch 1  | loss: 0.62946 | train_logloss: 0.63984 | valid_logloss: 0.63003 |  0:00:06s\n",
      "epoch 2  | loss: 0.62045 | train_logloss: 0.64003 | valid_logloss: 0.62979 |  0:00:10s\n",
      "epoch 3  | loss: 0.61566 | train_logloss: 0.63207 | valid_logloss: 0.62059 |  0:00:14s\n",
      "epoch 4  | loss: 0.61522 | train_logloss: 0.62878 | valid_logloss: 0.61804 |  0:00:19s\n",
      "epoch 5  | loss: 0.61425 | train_logloss: 0.61946 | valid_logloss: 0.61124 |  0:00:23s\n",
      "epoch 6  | loss: 0.61482 | train_logloss: 0.62101 | valid_logloss: 0.61543 |  0:00:27s\n",
      "epoch 7  | loss: 0.61292 | train_logloss: 0.61572 | valid_logloss: 0.60796 |  0:00:30s\n",
      "epoch 8  | loss: 0.61479 | train_logloss: 0.61301 | valid_logloss: 0.60448 |  0:00:34s\n",
      "epoch 9  | loss: 0.61481 | train_logloss: 0.61695 | valid_logloss: 0.60831 |  0:00:38s\n",
      "epoch 10 | loss: 0.61623 | train_logloss: 0.61388 | valid_logloss: 0.60613 |  0:00:41s\n",
      "epoch 11 | loss: 0.61168 | train_logloss: 0.61229 | valid_logloss: 0.60493 |  0:00:45s\n",
      "epoch 12 | loss: 0.61192 | train_logloss: 0.61268 | valid_logloss: 0.60611 |  0:00:49s\n",
      "epoch 13 | loss: 0.61294 | train_logloss: 0.61224 | valid_logloss: 0.60284 |  0:00:53s\n",
      "epoch 14 | loss: 0.61502 | train_logloss: 0.61056 | valid_logloss: 0.60408 |  0:00:58s\n",
      "epoch 15 | loss: 0.61478 | train_logloss: 0.61253 | valid_logloss: 0.60338 |  0:01:05s\n",
      "epoch 16 | loss: 0.61439 | train_logloss: 0.61052 | valid_logloss: 0.60246 |  0:01:10s\n",
      "epoch 17 | loss: 0.61283 | train_logloss: 0.61079 | valid_logloss: 0.60321 |  0:01:16s\n",
      "epoch 18 | loss: 0.61445 | train_logloss: 0.61117 | valid_logloss: 0.60256 |  0:01:22s\n",
      "epoch 19 | loss: 0.61341 | train_logloss: 0.61042 | valid_logloss: 0.60223 |  0:01:27s\n",
      "epoch 20 | loss: 0.61244 | train_logloss: 0.61046 | valid_logloss: 0.60118 |  0:01:33s\n",
      "epoch 21 | loss: 0.61231 | train_logloss: 0.61293 | valid_logloss: 0.6047  |  0:01:39s\n",
      "epoch 22 | loss: 0.61545 | train_logloss: 0.60959 | valid_logloss: 0.60217 |  0:01:44s\n",
      "epoch 23 | loss: 0.61009 | train_logloss: 0.60945 | valid_logloss: 0.60222 |  0:01:49s\n",
      "epoch 24 | loss: 0.6122  | train_logloss: 0.61026 | valid_logloss: 0.6035  |  0:01:55s\n",
      "epoch 25 | loss: 0.61259 | train_logloss: 0.61082 | valid_logloss: 0.60437 |  0:02:01s\n",
      "epoch 26 | loss: 0.61498 | train_logloss: 0.61091 | valid_logloss: 0.6035  |  0:02:07s\n",
      "epoch 27 | loss: 0.61181 | train_logloss: 0.60892 | valid_logloss: 0.60294 |  0:02:12s\n",
      "epoch 28 | loss: 0.61248 | train_logloss: 0.61065 | valid_logloss: 0.60324 |  0:02:16s\n",
      "epoch 29 | loss: 0.61067 | train_logloss: 0.60892 | valid_logloss: 0.60099 |  0:02:19s\n",
      "epoch 30 | loss: 0.60968 | train_logloss: 0.60863 | valid_logloss: 0.60154 |  0:02:22s\n",
      "epoch 31 | loss: 0.61148 | train_logloss: 0.60908 | valid_logloss: 0.60273 |  0:02:25s\n",
      "epoch 32 | loss: 0.60916 | train_logloss: 0.60792 | valid_logloss: 0.60272 |  0:02:29s\n",
      "epoch 33 | loss: 0.60882 | train_logloss: 0.60843 | valid_logloss: 0.60326 |  0:02:31s\n",
      "epoch 34 | loss: 0.61018 | train_logloss: 0.60879 | valid_logloss: 0.60324 |  0:02:35s\n",
      "epoch 35 | loss: 0.61127 | train_logloss: 0.60988 | valid_logloss: 0.60295 |  0:02:38s\n",
      "epoch 36 | loss: 0.60719 | train_logloss: 0.60839 | valid_logloss: 0.60334 |  0:02:41s\n",
      "epoch 37 | loss: 0.60753 | train_logloss: 0.60853 | valid_logloss: 0.60383 |  0:02:45s\n",
      "epoch 38 | loss: 0.61147 | train_logloss: 0.60835 | valid_logloss: 0.60204 |  0:02:48s\n",
      "epoch 39 | loss: 0.61004 | train_logloss: 0.60815 | valid_logloss: 0.60073 |  0:02:52s\n",
      "epoch 40 | loss: 0.60878 | train_logloss: 0.60722 | valid_logloss: 0.6026  |  0:02:55s\n",
      "epoch 41 | loss: 0.60971 | train_logloss: 0.60605 | valid_logloss: 0.60404 |  0:02:58s\n",
      "epoch 42 | loss: 0.60824 | train_logloss: 0.6065  | valid_logloss: 0.60301 |  0:03:02s\n",
      "epoch 43 | loss: 0.60889 | train_logloss: 0.60816 | valid_logloss: 0.60395 |  0:03:05s\n",
      "epoch 44 | loss: 0.60876 | train_logloss: 0.60712 | valid_logloss: 0.60446 |  0:03:08s\n",
      "epoch 45 | loss: 0.61028 | train_logloss: 0.60857 | valid_logloss: 0.60412 |  0:03:12s\n",
      "epoch 46 | loss: 0.61113 | train_logloss: 0.60929 | valid_logloss: 0.60486 |  0:03:15s\n",
      "epoch 47 | loss: 0.61002 | train_logloss: 0.60879 | valid_logloss: 0.60243 |  0:03:22s\n",
      "epoch 48 | loss: 0.61032 | train_logloss: 0.60822 | valid_logloss: 0.60063 |  0:03:31s\n",
      "epoch 49 | loss: 0.61063 | train_logloss: 0.60794 | valid_logloss: 0.60177 |  0:03:38s\n",
      "epoch 50 | loss: 0.60912 | train_logloss: 0.60775 | valid_logloss: 0.60109 |  0:03:44s\n",
      "epoch 51 | loss: 0.61062 | train_logloss: 0.60869 | valid_logloss: 0.6016  |  0:03:51s\n",
      "epoch 52 | loss: 0.61117 | train_logloss: 0.60798 | valid_logloss: 0.60026 |  0:03:59s\n",
      "epoch 53 | loss: 0.61187 | train_logloss: 0.60832 | valid_logloss: 0.6     |  0:04:05s\n",
      "epoch 54 | loss: 0.61001 | train_logloss: 0.609   | valid_logloss: 0.59962 |  0:04:12s\n",
      "epoch 55 | loss: 0.61217 | train_logloss: 0.60869 | valid_logloss: 0.59982 |  0:04:20s\n",
      "epoch 56 | loss: 0.60996 | train_logloss: 0.60884 | valid_logloss: 0.59843 |  0:04:23s\n",
      "epoch 57 | loss: 0.60842 | train_logloss: 0.60906 | valid_logloss: 0.59972 |  0:04:27s\n",
      "epoch 58 | loss: 0.61035 | train_logloss: 0.60818 | valid_logloss: 0.60154 |  0:04:30s\n",
      "epoch 59 | loss: 0.61303 | train_logloss: 0.60883 | valid_logloss: 0.6024  |  0:04:34s\n",
      "epoch 60 | loss: 0.60944 | train_logloss: 0.60877 | valid_logloss: 0.60224 |  0:04:37s\n",
      "epoch 61 | loss: 0.60955 | train_logloss: 0.60776 | valid_logloss: 0.60229 |  0:04:41s\n",
      "epoch 62 | loss: 0.60791 | train_logloss: 0.60702 | valid_logloss: 0.6013  |  0:04:44s\n",
      "epoch 63 | loss: 0.61007 | train_logloss: 0.60697 | valid_logloss: 0.60212 |  0:04:47s\n",
      "epoch 64 | loss: 0.60801 | train_logloss: 0.60731 | valid_logloss: 0.60136 |  0:04:50s\n",
      "epoch 65 | loss: 0.60879 | train_logloss: 0.60672 | valid_logloss: 0.6024  |  0:04:54s\n",
      "epoch 66 | loss: 0.60766 | train_logloss: 0.60547 | valid_logloss: 0.60064 |  0:04:57s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 56 and best_valid_logloss = 0.59843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "col_target = \"health\"\n",
    "\n",
    "# train tabnet\n",
    "clf_lst, oof_pred = train_tabnet(train, cols_exp, cols_cat, col_target)\n",
    "\n",
    "# normalization for test\n",
    "x_train = train[cols_exp].to_numpy()\n",
    "x_test = test[cols_exp].to_numpy()\n",
    "cols_num_idxs = [i for i, c in enumerate(cols_exp) if not c in cols_cat]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train[:, cols_num_idxs])\n",
    "x_test[:, cols_num_idxs] = scaler.transform(x_test[:, cols_num_idxs])\n",
    "\n",
    "# predict test with CV ensemble\n",
    "y_test_pred = predict_test(x_test, clf_lst)\n",
    "\n",
    "# record\n",
    "oof_pred_df = pl.DataFrame(oof_pred, schema=[f\"health_is_{h}\" for h in range(3)])\n",
    "test_pred_df = pl.DataFrame(y_test_pred, schema=[f\"health_is_{h}\" for h in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "oof_pred_df.write_csv(f\"pred/oof_pred_tabnet_{feat}.csv\")\n",
    "test_pred_df.write_csv(f\"pred/test_pred_tabnet_{feat}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d00ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3ba5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
