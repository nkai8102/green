{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2ef3d0-c8bc-4511-934e-7205c5825dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4d8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"feat00\"\n",
    "train = pl.read_csv(f\"feat/feat_train_{feat}.csv\")\n",
    "test = pl.read_csv(f\"feat/feat_test_{feat}.csv\")\n",
    "train_origin = pl.read_csv(\"data/train.csv\").rename({\"\": \"idx\"})\n",
    "\n",
    "# 説明変数のカラム\n",
    "cols_exp = [c for c in test.columns if c != \"idx\"]\n",
    "\n",
    "# カテゴリ特徴量のカラム\n",
    "cols_notcat = ['idx', 'created_at', 'tree_dbh']\n",
    "cols_cat = [c for c in test.columns if not c in cols_notcat] # カテゴリ特徴量\n",
    "\n",
    "for col in cols_cat:\n",
    "    # もし欠損値があればその特徴量のユニーク数(欠損値を除く)で埋める（ordinal encoding前提）\n",
    "    num_null = train[col].n_unique() - 1\n",
    "    train = train.with_columns(train[col].fill_null(num_null))\n",
    "    test = test.with_columns(test[col].fill_null(num_null))\n",
    "\n",
    "# 100以上のユニーク数をもつ特徴量を削除（train/valid split時にordinal encodingの連番の整合性が崩れるため）\n",
    "# ※ pretrain実施しない場合はコメントアウトする\n",
    "cols_exp = [c for c in cols_exp if not c in (\"boro_ct\", \"spc_latin\", \"nta\")]\n",
    "cols_cat = [c for c in cols_cat if not c in (\"boro_ct\", \"spc_latin\", \"nta\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990efad",
   "metadata": {},
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3c42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabnet(train, cols_exp, cols_cat, col_target, params=None):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    params_add = {\"device_name\": \"cpu\", \"seed\": 0}\n",
    "    params |= params_add\n",
    "\n",
    "    x = train[cols_exp].to_numpy()\n",
    "    y = train[col_target].to_numpy()\n",
    "\n",
    "    # cols_expにおけるカテゴリ変数のインデックス\n",
    "    cols_cat_idxs = [i for i, c in enumerate(cols_exp) if c in cols_cat]\n",
    "    cols_num_idxs = [i for i, c in enumerate(cols_exp) if not c in cols_cat]\n",
    "    cols_cat_dims = train.approx_n_unique()[cols_exp].to_numpy().ravel()[cols_cat_idxs]\n",
    "\n",
    "    # K-fold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    y_valid_pred_lst = []\n",
    "    idx_valid_lst = []\n",
    "    clf_lst = []\n",
    "\n",
    "    # cross validation\n",
    "    for fold, (idx_train, idx_valid) in enumerate(kf.split(x)):\n",
    "        print(\"fold\", fold)\n",
    "        x_train = x[idx_train, :]\n",
    "        x_valid = x[idx_valid, :]\n",
    "        y_train = y[idx_train]\n",
    "        y_valid = y[idx_valid]\n",
    "        \n",
    "        # normalization\n",
    "        scaler = StandardScaler()\n",
    "        x_train[:, cols_num_idxs] = scaler.fit_transform(x_train[:, cols_num_idxs])\n",
    "        x_valid[:, cols_num_idxs] = scaler.transform(x_valid[:, cols_num_idxs])\n",
    "\n",
    "        # modeling\n",
    "        pretrainer = TabNetPretrainer(**params)\n",
    "        pretrainer.fit(x_train, eval_set=[x_valid])\n",
    "        clf = TabNetClassifier(**params, cat_idxs=cols_cat_idxs, cat_dims=cols_cat_dims)\n",
    "        clf.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=[\"logloss\"], \n",
    "            from_unsupervised=pretrainer\n",
    "        )   \n",
    "        \n",
    "        # oof\n",
    "        y_valid_pred = clf.predict_proba(x_valid)\n",
    "        y_valid_pred_lst.append(y_valid_pred)\n",
    "        idx_valid_lst.append(idx_valid)\n",
    "        clf_lst.append(clf)\n",
    "\n",
    "    idx_valid = np.hstack(idx_valid_lst)\n",
    "    y_valid_pred = np.vstack(y_valid_pred_lst)\n",
    "    oof_pred = y_valid_pred[np.argsort(idx_valid)]\n",
    "\n",
    "    return clf_lst, oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960ae40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(x_test, clf_lst):\n",
    "    y_test_pred_lst = []\n",
    "\n",
    "    for clf in clf_lst:\n",
    "        y_test_pred = clf.predict_proba(x_test)\n",
    "        y_test_pred_lst.append(y_test_pred)\n",
    "\n",
    "    y_test_pred = np.mean(y_test_pred_lst, axis=0)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4adc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 26.76198| val_0_unsup_loss_numpy: 20.42473030090332|  0:00:00s\n",
      "epoch 1  | loss: 4.08857 | val_0_unsup_loss_numpy: 10.79423999786377|  0:00:01s\n",
      "epoch 2  | loss: 1.95025 | val_0_unsup_loss_numpy: 5.092899799346924|  0:00:02s\n",
      "epoch 3  | loss: 1.46901 | val_0_unsup_loss_numpy: 3.2443299293518066|  0:00:03s\n",
      "epoch 4  | loss: 1.27537 | val_0_unsup_loss_numpy: 1.1792399883270264|  0:00:04s\n",
      "epoch 5  | loss: 1.1298  | val_0_unsup_loss_numpy: 1.4030499458312988|  0:00:05s\n",
      "epoch 6  | loss: 1.08237 | val_0_unsup_loss_numpy: 1.2402499914169312|  0:00:06s\n",
      "epoch 7  | loss: 1.20298 | val_0_unsup_loss_numpy: 1.3411699533462524|  0:00:07s\n",
      "epoch 8  | loss: 1.13437 | val_0_unsup_loss_numpy: 1.1023600101470947|  0:00:08s\n",
      "epoch 9  | loss: 1.00452 | val_0_unsup_loss_numpy: 1.2569999694824219|  0:00:09s\n",
      "epoch 10 | loss: 1.02615 | val_0_unsup_loss_numpy: 1.0763499736785889|  0:00:09s\n",
      "epoch 11 | loss: 0.99021 | val_0_unsup_loss_numpy: 0.9975200295448303|  0:00:10s\n",
      "epoch 12 | loss: 1.00271 | val_0_unsup_loss_numpy: 1.161270022392273|  0:00:11s\n",
      "epoch 13 | loss: 0.9899  | val_0_unsup_loss_numpy: 1.375480055809021|  0:00:12s\n",
      "epoch 14 | loss: 1.01385 | val_0_unsup_loss_numpy: 1.583299994468689|  0:00:13s\n",
      "epoch 15 | loss: 1.18306 | val_0_unsup_loss_numpy: 5.377620220184326|  0:00:14s\n",
      "epoch 16 | loss: 1.71539 | val_0_unsup_loss_numpy: 5.076159954071045|  0:00:15s\n",
      "epoch 17 | loss: 1.58547 | val_0_unsup_loss_numpy: 4.400290012359619|  0:00:16s\n",
      "epoch 18 | loss: 1.32595 | val_0_unsup_loss_numpy: 1.1741000413894653|  0:00:17s\n",
      "epoch 19 | loss: 1.2099  | val_0_unsup_loss_numpy: 3.2756600379943848|  0:00:18s\n",
      "epoch 20 | loss: 1.32795 | val_0_unsup_loss_numpy: 3.9267399311065674|  0:00:18s\n",
      "epoch 21 | loss: 0.97672 | val_0_unsup_loss_numpy: 0.9183400273323059|  0:00:19s\n",
      "epoch 22 | loss: 0.93309 | val_0_unsup_loss_numpy: 0.9080299735069275|  0:00:20s\n",
      "epoch 23 | loss: 1.00153 | val_0_unsup_loss_numpy: 0.9425399899482727|  0:00:21s\n",
      "epoch 24 | loss: 0.93382 | val_0_unsup_loss_numpy: 1.724020004272461|  0:00:22s\n",
      "epoch 25 | loss: 0.97879 | val_0_unsup_loss_numpy: 0.8868200182914734|  0:00:23s\n",
      "epoch 26 | loss: 0.90603 | val_0_unsup_loss_numpy: 1.1196199655532837|  0:00:24s\n",
      "epoch 27 | loss: 0.91671 | val_0_unsup_loss_numpy: 0.9485599994659424|  0:00:25s\n",
      "epoch 28 | loss: 0.94609 | val_0_unsup_loss_numpy: 0.9111199975013733|  0:00:26s\n",
      "epoch 29 | loss: 0.91659 | val_0_unsup_loss_numpy: 1.204259991645813|  0:00:27s\n",
      "epoch 30 | loss: 0.92242 | val_0_unsup_loss_numpy: 0.9050899744033813|  0:00:28s\n",
      "epoch 31 | loss: 0.91662 | val_0_unsup_loss_numpy: 0.8040400147438049|  0:00:28s\n",
      "epoch 32 | loss: 0.92272 | val_0_unsup_loss_numpy: 0.852180004119873|  0:00:29s\n",
      "epoch 33 | loss: 0.8971  | val_0_unsup_loss_numpy: 0.9813600182533264|  0:00:30s\n",
      "epoch 34 | loss: 0.91833 | val_0_unsup_loss_numpy: 0.8852099776268005|  0:00:31s\n",
      "epoch 35 | loss: 0.89303 | val_0_unsup_loss_numpy: 1.31427001953125|  0:00:32s\n",
      "epoch 36 | loss: 0.92448 | val_0_unsup_loss_numpy: 1.4248100519180298|  0:00:33s\n",
      "epoch 37 | loss: 0.94312 | val_0_unsup_loss_numpy: 1.0547499656677246|  0:00:34s\n",
      "epoch 38 | loss: 0.90889 | val_0_unsup_loss_numpy: 1.345229983329773|  0:00:35s\n",
      "epoch 39 | loss: 1.01869 | val_0_unsup_loss_numpy: 0.7760099768638611|  0:00:36s\n",
      "epoch 40 | loss: 0.94758 | val_0_unsup_loss_numpy: 0.840910017490387|  0:00:37s\n",
      "epoch 41 | loss: 1.04346 | val_0_unsup_loss_numpy: 1.724679946899414|  0:00:38s\n",
      "epoch 42 | loss: 0.98839 | val_0_unsup_loss_numpy: 2.3728699684143066|  0:00:39s\n",
      "epoch 43 | loss: 1.0801  | val_0_unsup_loss_numpy: 1.590109944343567|  0:00:40s\n",
      "epoch 44 | loss: 0.96633 | val_0_unsup_loss_numpy: 0.8698499798774719|  0:00:41s\n",
      "epoch 45 | loss: 0.95451 | val_0_unsup_loss_numpy: 0.8757100105285645|  0:00:42s\n",
      "epoch 46 | loss: 1.02662 | val_0_unsup_loss_numpy: 1.6793400049209595|  0:00:43s\n",
      "epoch 47 | loss: 0.97852 | val_0_unsup_loss_numpy: 1.1825100183486938|  0:00:44s\n",
      "epoch 48 | loss: 0.9151  | val_0_unsup_loss_numpy: 0.8372399806976318|  0:00:44s\n",
      "epoch 49 | loss: 0.92304 | val_0_unsup_loss_numpy: 1.029829978942871|  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_unsup_loss_numpy = 0.7760099768638611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.74412 | train_logloss: 0.66233 | valid_logloss: 0.67185 |  0:00:01s\n",
      "epoch 1  | loss: 0.62741 | train_logloss: 0.63318 | valid_logloss: 0.64351 |  0:00:02s\n",
      "epoch 2  | loss: 0.6168  | train_logloss: 0.61387 | valid_logloss: 0.61974 |  0:00:03s\n",
      "epoch 3  | loss: 0.61381 | train_logloss: 0.61287 | valid_logloss: 0.61755 |  0:00:04s\n",
      "epoch 4  | loss: 0.61177 | train_logloss: 0.61425 | valid_logloss: 0.62027 |  0:00:05s\n",
      "epoch 5  | loss: 0.61214 | train_logloss: 0.61341 | valid_logloss: 0.61928 |  0:00:06s\n",
      "epoch 6  | loss: 0.61252 | train_logloss: 0.61258 | valid_logloss: 0.61788 |  0:00:07s\n",
      "epoch 7  | loss: 0.61112 | train_logloss: 0.60901 | valid_logloss: 0.6166  |  0:00:08s\n",
      "epoch 8  | loss: 0.61022 | train_logloss: 0.60804 | valid_logloss: 0.61677 |  0:00:09s\n",
      "epoch 9  | loss: 0.61043 | train_logloss: 0.60974 | valid_logloss: 0.61802 |  0:00:10s\n",
      "epoch 10 | loss: 0.61256 | train_logloss: 0.61022 | valid_logloss: 0.61836 |  0:00:12s\n",
      "epoch 11 | loss: 0.61066 | train_logloss: 0.61047 | valid_logloss: 0.61821 |  0:00:13s\n",
      "epoch 12 | loss: 0.61082 | train_logloss: 0.60792 | valid_logloss: 0.61397 |  0:00:14s\n",
      "epoch 13 | loss: 0.60816 | train_logloss: 0.60699 | valid_logloss: 0.61225 |  0:00:15s\n",
      "epoch 14 | loss: 0.60801 | train_logloss: 0.60755 | valid_logloss: 0.61227 |  0:00:16s\n",
      "epoch 15 | loss: 0.6086  | train_logloss: 0.60729 | valid_logloss: 0.61342 |  0:00:17s\n",
      "epoch 16 | loss: 0.61035 | train_logloss: 0.60646 | valid_logloss: 0.61161 |  0:00:18s\n",
      "epoch 17 | loss: 0.60776 | train_logloss: 0.60652 | valid_logloss: 0.6115  |  0:00:19s\n",
      "epoch 18 | loss: 0.60607 | train_logloss: 0.60596 | valid_logloss: 0.61205 |  0:00:20s\n",
      "epoch 19 | loss: 0.60719 | train_logloss: 0.60776 | valid_logloss: 0.61596 |  0:00:22s\n",
      "epoch 20 | loss: 0.61246 | train_logloss: 0.6062  | valid_logloss: 0.61432 |  0:00:23s\n",
      "epoch 21 | loss: 0.60924 | train_logloss: 0.60618 | valid_logloss: 0.61502 |  0:00:24s\n",
      "epoch 22 | loss: 0.60767 | train_logloss: 0.60612 | valid_logloss: 0.61303 |  0:00:25s\n",
      "epoch 23 | loss: 0.60822 | train_logloss: 0.60577 | valid_logloss: 0.61227 |  0:00:26s\n",
      "epoch 24 | loss: 0.60809 | train_logloss: 0.60623 | valid_logloss: 0.61367 |  0:00:27s\n",
      "epoch 25 | loss: 0.6068  | train_logloss: 0.60626 | valid_logloss: 0.61203 |  0:00:28s\n",
      "epoch 26 | loss: 0.60883 | train_logloss: 0.60536 | valid_logloss: 0.61355 |  0:00:29s\n",
      "epoch 27 | loss: 0.60876 | train_logloss: 0.60589 | valid_logloss: 0.6131  |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_valid_logloss = 0.6115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 44.57027| val_0_unsup_loss_numpy: 39.23863983154297|  0:00:00s\n",
      "epoch 1  | loss: 3.43539 | val_0_unsup_loss_numpy: 6.510300159454346|  0:00:01s\n",
      "epoch 2  | loss: 1.66558 | val_0_unsup_loss_numpy: 15.489069938659668|  0:00:02s\n",
      "epoch 3  | loss: 1.27852 | val_0_unsup_loss_numpy: 2.172830104827881|  0:00:03s\n",
      "epoch 4  | loss: 1.24417 | val_0_unsup_loss_numpy: 1.311750054359436|  0:00:04s\n",
      "epoch 5  | loss: 1.19865 | val_0_unsup_loss_numpy: 1.9211100339889526|  0:00:05s\n",
      "epoch 6  | loss: 1.14775 | val_0_unsup_loss_numpy: 1.2566399574279785|  0:00:06s\n",
      "epoch 7  | loss: 1.1991  | val_0_unsup_loss_numpy: 1.7197599411010742|  0:00:07s\n",
      "epoch 8  | loss: 1.33253 | val_0_unsup_loss_numpy: 6.3994598388671875|  0:00:08s\n",
      "epoch 9  | loss: 1.19769 | val_0_unsup_loss_numpy: 1.115339994430542|  0:00:08s\n",
      "epoch 10 | loss: 0.99937 | val_0_unsup_loss_numpy: 1.0732500553131104|  0:00:09s\n",
      "epoch 11 | loss: 1.00555 | val_0_unsup_loss_numpy: 1.053760051727295|  0:00:10s\n",
      "epoch 12 | loss: 1.02236 | val_0_unsup_loss_numpy: 1.7133100032806396|  0:00:11s\n",
      "epoch 13 | loss: 1.03136 | val_0_unsup_loss_numpy: 1.008579969406128|  0:00:12s\n",
      "epoch 14 | loss: 0.97929 | val_0_unsup_loss_numpy: 1.1375999450683594|  0:00:13s\n",
      "epoch 15 | loss: 1.01585 | val_0_unsup_loss_numpy: 1.0255600214004517|  0:00:14s\n",
      "epoch 16 | loss: 1.04464 | val_0_unsup_loss_numpy: 1.032789945602417|  0:00:15s\n",
      "epoch 17 | loss: 0.99091 | val_0_unsup_loss_numpy: 0.9725099802017212|  0:00:16s\n",
      "epoch 18 | loss: 1.06271 | val_0_unsup_loss_numpy: 0.9983999729156494|  0:00:17s\n",
      "epoch 19 | loss: 1.01816 | val_0_unsup_loss_numpy: 1.0587199926376343|  0:00:17s\n",
      "epoch 20 | loss: 1.0211  | val_0_unsup_loss_numpy: 1.0246299505233765|  0:00:18s\n",
      "epoch 21 | loss: 0.99856 | val_0_unsup_loss_numpy: 1.0389000177383423|  0:00:19s\n",
      "epoch 22 | loss: 1.12642 | val_0_unsup_loss_numpy: 1.3908400535583496|  0:00:20s\n",
      "epoch 23 | loss: 1.27131 | val_0_unsup_loss_numpy: 5.820099830627441|  0:00:21s\n",
      "epoch 24 | loss: 1.21748 | val_0_unsup_loss_numpy: 1.068660020828247|  0:00:22s\n",
      "epoch 25 | loss: 1.01079 | val_0_unsup_loss_numpy: 1.6533199548721313|  0:00:23s\n",
      "epoch 26 | loss: 1.12459 | val_0_unsup_loss_numpy: 1.015910029411316|  0:00:24s\n",
      "epoch 27 | loss: 1.17197 | val_0_unsup_loss_numpy: 2.3574600219726562|  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_unsup_loss_numpy = 0.9725099802017212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70838 | train_logloss: 0.68116 | valid_logloss: 0.68349 |  0:00:01s\n",
      "epoch 1  | loss: 0.6201  | train_logloss: 0.62628 | valid_logloss: 0.63706 |  0:00:02s\n",
      "epoch 2  | loss: 0.6135  | train_logloss: 0.61671 | valid_logloss: 0.62663 |  0:00:03s\n",
      "epoch 3  | loss: 0.61093 | train_logloss: 0.61271 | valid_logloss: 0.62107 |  0:00:04s\n",
      "epoch 4  | loss: 0.61016 | train_logloss: 0.60978 | valid_logloss: 0.62184 |  0:00:05s\n",
      "epoch 5  | loss: 0.60988 | train_logloss: 0.6104  | valid_logloss: 0.62135 |  0:00:06s\n",
      "epoch 6  | loss: 0.61089 | train_logloss: 0.60916 | valid_logloss: 0.62    |  0:00:07s\n",
      "epoch 7  | loss: 0.60917 | train_logloss: 0.60842 | valid_logloss: 0.61975 |  0:00:08s\n",
      "epoch 8  | loss: 0.609   | train_logloss: 0.6087  | valid_logloss: 0.62054 |  0:00:09s\n",
      "epoch 9  | loss: 0.61044 | train_logloss: 0.60788 | valid_logloss: 0.61993 |  0:00:11s\n",
      "epoch 10 | loss: 0.60801 | train_logloss: 0.60836 | valid_logloss: 0.62118 |  0:00:12s\n",
      "epoch 11 | loss: 0.60731 | train_logloss: 0.60692 | valid_logloss: 0.61852 |  0:00:13s\n",
      "epoch 12 | loss: 0.60826 | train_logloss: 0.60677 | valid_logloss: 0.61958 |  0:00:14s\n",
      "epoch 13 | loss: 0.60728 | train_logloss: 0.60938 | valid_logloss: 0.6234  |  0:00:15s\n",
      "epoch 14 | loss: 0.60824 | train_logloss: 0.60615 | valid_logloss: 0.61741 |  0:00:16s\n",
      "epoch 15 | loss: 0.60678 | train_logloss: 0.60649 | valid_logloss: 0.61689 |  0:00:17s\n",
      "epoch 16 | loss: 0.60892 | train_logloss: 0.60845 | valid_logloss: 0.62162 |  0:00:18s\n",
      "epoch 17 | loss: 0.60911 | train_logloss: 0.60621 | valid_logloss: 0.61772 |  0:00:19s\n",
      "epoch 18 | loss: 0.61033 | train_logloss: 0.6065  | valid_logloss: 0.61703 |  0:00:21s\n",
      "epoch 19 | loss: 0.60927 | train_logloss: 0.60684 | valid_logloss: 0.61839 |  0:00:22s\n",
      "epoch 20 | loss: 0.61035 | train_logloss: 0.60763 | valid_logloss: 0.61983 |  0:00:23s\n",
      "epoch 21 | loss: 0.60874 | train_logloss: 0.60573 | valid_logloss: 0.61639 |  0:00:24s\n",
      "epoch 22 | loss: 0.60966 | train_logloss: 0.60681 | valid_logloss: 0.6188  |  0:00:25s\n",
      "epoch 23 | loss: 0.61078 | train_logloss: 0.6069  | valid_logloss: 0.61662 |  0:00:26s\n",
      "epoch 24 | loss: 0.60845 | train_logloss: 0.608   | valid_logloss: 0.6183  |  0:00:27s\n",
      "epoch 25 | loss: 0.60949 | train_logloss: 0.60842 | valid_logloss: 0.62027 |  0:00:28s\n",
      "epoch 26 | loss: 0.60571 | train_logloss: 0.60624 | valid_logloss: 0.61908 |  0:00:29s\n",
      "epoch 27 | loss: 0.60899 | train_logloss: 0.60584 | valid_logloss: 0.61792 |  0:00:31s\n",
      "epoch 28 | loss: 0.60716 | train_logloss: 0.60558 | valid_logloss: 0.61907 |  0:00:32s\n",
      "epoch 29 | loss: 0.60468 | train_logloss: 0.60624 | valid_logloss: 0.61918 |  0:00:33s\n",
      "epoch 30 | loss: 0.60942 | train_logloss: 0.60617 | valid_logloss: 0.61839 |  0:00:34s\n",
      "epoch 31 | loss: 0.60712 | train_logloss: 0.60462 | valid_logloss: 0.61779 |  0:00:35s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_valid_logloss = 0.61639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 36.11608| val_0_unsup_loss_numpy: 75.12001037597656|  0:00:00s\n",
      "epoch 1  | loss: 3.49095 | val_0_unsup_loss_numpy: 55.85118103027344|  0:00:01s\n",
      "epoch 2  | loss: 1.65772 | val_0_unsup_loss_numpy: 7.563960075378418|  0:00:02s\n",
      "epoch 3  | loss: 1.47043 | val_0_unsup_loss_numpy: 2.963409900665283|  0:00:03s\n",
      "epoch 4  | loss: 1.34936 | val_0_unsup_loss_numpy: 3.136539936065674|  0:00:04s\n",
      "epoch 5  | loss: 1.22492 | val_0_unsup_loss_numpy: 2.168299913406372|  0:00:05s\n",
      "epoch 6  | loss: 1.13605 | val_0_unsup_loss_numpy: 1.23239004611969|  0:00:06s\n",
      "epoch 7  | loss: 1.09811 | val_0_unsup_loss_numpy: 1.046180009841919|  0:00:07s\n",
      "epoch 8  | loss: 1.09654 | val_0_unsup_loss_numpy: 1.6035300493240356|  0:00:08s\n",
      "epoch 9  | loss: 1.09853 | val_0_unsup_loss_numpy: 1.6876599788665771|  0:00:09s\n",
      "epoch 10 | loss: 1.08077 | val_0_unsup_loss_numpy: 1.2708200216293335|  0:00:09s\n",
      "epoch 11 | loss: 1.36933 | val_0_unsup_loss_numpy: 6.931879997253418|  0:00:10s\n",
      "epoch 12 | loss: 1.07511 | val_0_unsup_loss_numpy: 0.9894899725914001|  0:00:11s\n",
      "epoch 13 | loss: 1.01692 | val_0_unsup_loss_numpy: 1.0934699773788452|  0:00:12s\n",
      "epoch 14 | loss: 0.99257 | val_0_unsup_loss_numpy: 1.1356099843978882|  0:00:13s\n",
      "epoch 15 | loss: 0.98055 | val_0_unsup_loss_numpy: 1.2428200244903564|  0:00:14s\n",
      "epoch 16 | loss: 0.99556 | val_0_unsup_loss_numpy: 1.0642600059509277|  0:00:15s\n",
      "epoch 17 | loss: 0.96164 | val_0_unsup_loss_numpy: 1.057979941368103|  0:00:16s\n",
      "epoch 18 | loss: 0.98216 | val_0_unsup_loss_numpy: 2.4132399559020996|  0:00:17s\n",
      "epoch 19 | loss: 1.01383 | val_0_unsup_loss_numpy: 2.681380033493042|  0:00:17s\n",
      "epoch 20 | loss: 1.1851  | val_0_unsup_loss_numpy: 0.9342600107192993|  0:00:18s\n",
      "epoch 21 | loss: 1.35539 | val_0_unsup_loss_numpy: 1.143720030784607|  0:00:19s\n",
      "epoch 22 | loss: 1.01779 | val_0_unsup_loss_numpy: 6.25324010848999|  0:00:20s\n",
      "epoch 23 | loss: 1.3204  | val_0_unsup_loss_numpy: 3.442660093307495|  0:00:21s\n",
      "epoch 24 | loss: 1.46427 | val_0_unsup_loss_numpy: 1.746880054473877|  0:00:22s\n",
      "epoch 25 | loss: 1.28239 | val_0_unsup_loss_numpy: 2.258929967880249|  0:00:23s\n",
      "epoch 26 | loss: 1.02565 | val_0_unsup_loss_numpy: 0.8835099935531616|  0:00:24s\n",
      "epoch 27 | loss: 0.91658 | val_0_unsup_loss_numpy: 0.9282000064849854|  0:00:25s\n",
      "epoch 28 | loss: 0.95141 | val_0_unsup_loss_numpy: 1.2530900239944458|  0:00:26s\n",
      "epoch 29 | loss: 0.93786 | val_0_unsup_loss_numpy: 0.9449800252914429|  0:00:27s\n",
      "epoch 30 | loss: 0.92601 | val_0_unsup_loss_numpy: 1.0489599704742432|  0:00:27s\n",
      "epoch 31 | loss: 0.95732 | val_0_unsup_loss_numpy: 0.8399999737739563|  0:00:28s\n",
      "epoch 32 | loss: 0.91208 | val_0_unsup_loss_numpy: 0.8341799974441528|  0:00:29s\n",
      "epoch 33 | loss: 0.97826 | val_0_unsup_loss_numpy: 0.8359100222587585|  0:00:30s\n",
      "epoch 34 | loss: 0.93142 | val_0_unsup_loss_numpy: 1.4793699979782104|  0:00:31s\n",
      "epoch 35 | loss: 0.90312 | val_0_unsup_loss_numpy: 0.8382899761199951|  0:00:32s\n",
      "epoch 36 | loss: 0.86867 | val_0_unsup_loss_numpy: 0.8312699794769287|  0:00:33s\n",
      "epoch 37 | loss: 0.8987  | val_0_unsup_loss_numpy: 0.9223700165748596|  0:00:34s\n",
      "epoch 38 | loss: 0.88891 | val_0_unsup_loss_numpy: 0.8695399761199951|  0:00:35s\n",
      "epoch 39 | loss: 0.89173 | val_0_unsup_loss_numpy: 0.9202499985694885|  0:00:36s\n",
      "epoch 40 | loss: 0.92052 | val_0_unsup_loss_numpy: 1.6206200122833252|  0:00:36s\n",
      "epoch 41 | loss: 0.91979 | val_0_unsup_loss_numpy: 0.8620700240135193|  0:00:37s\n",
      "epoch 42 | loss: 0.9107  | val_0_unsup_loss_numpy: 0.8579199910163879|  0:00:38s\n",
      "epoch 43 | loss: 0.87558 | val_0_unsup_loss_numpy: 0.8072800040245056|  0:00:39s\n",
      "epoch 44 | loss: 0.91529 | val_0_unsup_loss_numpy: 1.0090899467468262|  0:00:40s\n",
      "epoch 45 | loss: 0.90183 | val_0_unsup_loss_numpy: 1.1684900522232056|  0:00:41s\n",
      "epoch 46 | loss: 0.90682 | val_0_unsup_loss_numpy: 1.6202199459075928|  0:00:42s\n",
      "epoch 47 | loss: 0.94336 | val_0_unsup_loss_numpy: 0.8823099732398987|  0:00:43s\n",
      "epoch 48 | loss: 0.88663 | val_0_unsup_loss_numpy: 0.8877099752426147|  0:00:44s\n",
      "epoch 49 | loss: 0.92548 | val_0_unsup_loss_numpy: 1.0645099878311157|  0:00:45s\n",
      "epoch 50 | loss: 0.86602 | val_0_unsup_loss_numpy: 0.7898399829864502|  0:00:45s\n",
      "epoch 51 | loss: 0.88064 | val_0_unsup_loss_numpy: 3.4360899925231934|  0:00:46s\n",
      "epoch 52 | loss: 0.99492 | val_0_unsup_loss_numpy: 0.8726000189781189|  0:00:47s\n",
      "epoch 53 | loss: 0.86415 | val_0_unsup_loss_numpy: 1.6090199947357178|  0:00:48s\n",
      "epoch 54 | loss: 0.92052 | val_0_unsup_loss_numpy: 0.7839999794960022|  0:00:49s\n",
      "epoch 55 | loss: 0.86876 | val_0_unsup_loss_numpy: 0.7792900204658508|  0:00:50s\n",
      "epoch 56 | loss: 0.88111 | val_0_unsup_loss_numpy: 0.9893900156021118|  0:00:51s\n",
      "epoch 57 | loss: 0.90558 | val_0_unsup_loss_numpy: 1.1756600141525269|  0:00:52s\n",
      "epoch 58 | loss: 0.94125 | val_0_unsup_loss_numpy: 0.7778199911117554|  0:00:53s\n",
      "epoch 59 | loss: 0.89541 | val_0_unsup_loss_numpy: 0.8569599986076355|  0:00:54s\n",
      "epoch 60 | loss: 0.86115 | val_0_unsup_loss_numpy: 0.839169979095459|  0:00:55s\n",
      "epoch 61 | loss: 0.87927 | val_0_unsup_loss_numpy: 0.900160014629364|  0:00:55s\n",
      "epoch 62 | loss: 0.87636 | val_0_unsup_loss_numpy: 1.3249200582504272|  0:00:56s\n",
      "epoch 63 | loss: 1.01545 | val_0_unsup_loss_numpy: 0.9231299757957458|  0:00:57s\n",
      "epoch 64 | loss: 0.90006 | val_0_unsup_loss_numpy: 3.4943699836730957|  0:00:58s\n",
      "epoch 65 | loss: 0.9819  | val_0_unsup_loss_numpy: 0.9137899875640869|  0:00:59s\n",
      "epoch 66 | loss: 0.86798 | val_0_unsup_loss_numpy: 0.7698400020599365|  0:01:00s\n",
      "epoch 67 | loss: 0.88186 | val_0_unsup_loss_numpy: 0.7736200094223022|  0:01:01s\n",
      "epoch 68 | loss: 0.87504 | val_0_unsup_loss_numpy: 0.8341100215911865|  0:01:02s\n",
      "epoch 69 | loss: 0.85678 | val_0_unsup_loss_numpy: 0.8452399969100952|  0:01:03s\n",
      "epoch 70 | loss: 0.86875 | val_0_unsup_loss_numpy: 0.9081500172615051|  0:01:04s\n",
      "epoch 71 | loss: 0.85725 | val_0_unsup_loss_numpy: 0.8300899863243103|  0:01:04s\n",
      "epoch 72 | loss: 0.88224 | val_0_unsup_loss_numpy: 0.7632799744606018|  0:01:05s\n",
      "epoch 73 | loss: 0.87621 | val_0_unsup_loss_numpy: 0.8415399789810181|  0:01:06s\n",
      "epoch 74 | loss: 0.85651 | val_0_unsup_loss_numpy: 0.8184800148010254|  0:01:07s\n",
      "epoch 75 | loss: 0.85967 | val_0_unsup_loss_numpy: 0.8546000123023987|  0:01:08s\n",
      "epoch 76 | loss: 0.88923 | val_0_unsup_loss_numpy: 0.8426100015640259|  0:01:09s\n",
      "epoch 77 | loss: 0.8466  | val_0_unsup_loss_numpy: 0.7629200220108032|  0:01:10s\n",
      "epoch 78 | loss: 0.83007 | val_0_unsup_loss_numpy: 0.7664200067520142|  0:01:11s\n",
      "epoch 79 | loss: 0.86342 | val_0_unsup_loss_numpy: 0.7681800127029419|  0:01:12s\n",
      "epoch 80 | loss: 0.85507 | val_0_unsup_loss_numpy: 0.8253700137138367|  0:01:13s\n",
      "epoch 81 | loss: 0.82853 | val_0_unsup_loss_numpy: 0.821690022945404|  0:01:13s\n",
      "epoch 82 | loss: 0.86141 | val_0_unsup_loss_numpy: 0.76323002576828|  0:01:14s\n",
      "epoch 83 | loss: 0.85846 | val_0_unsup_loss_numpy: 0.7652999758720398|  0:01:15s\n",
      "epoch 84 | loss: 0.85295 | val_0_unsup_loss_numpy: 0.871940016746521|  0:01:16s\n",
      "epoch 85 | loss: 0.86241 | val_0_unsup_loss_numpy: 0.9049000144004822|  0:01:17s\n",
      "epoch 86 | loss: 0.86465 | val_0_unsup_loss_numpy: 0.7662000060081482|  0:01:18s\n",
      "epoch 87 | loss: 0.83253 | val_0_unsup_loss_numpy: 0.7614399790763855|  0:01:19s\n",
      "epoch 88 | loss: 0.88355 | val_0_unsup_loss_numpy: 0.8464099764823914|  0:01:20s\n",
      "epoch 89 | loss: 0.88027 | val_0_unsup_loss_numpy: 0.8505200147628784|  0:01:21s\n",
      "epoch 90 | loss: 0.85471 | val_0_unsup_loss_numpy: 0.7803699970245361|  0:01:21s\n",
      "epoch 91 | loss: 0.8614  | val_0_unsup_loss_numpy: 1.5829399824142456|  0:01:22s\n",
      "epoch 92 | loss: 0.96824 | val_0_unsup_loss_numpy: 0.7766600251197815|  0:01:23s\n",
      "epoch 93 | loss: 0.87856 | val_0_unsup_loss_numpy: 1.739109992980957|  0:01:24s\n",
      "epoch 94 | loss: 0.87919 | val_0_unsup_loss_numpy: 0.8179699778556824|  0:01:25s\n",
      "epoch 95 | loss: 0.86451 | val_0_unsup_loss_numpy: 0.8145899772644043|  0:01:26s\n",
      "epoch 96 | loss: 0.85085 | val_0_unsup_loss_numpy: 0.8108400106430054|  0:01:27s\n",
      "epoch 97 | loss: 0.82537 | val_0_unsup_loss_numpy: 0.8122299909591675|  0:01:28s\n",
      "\n",
      "Early stopping occurred at epoch 97 with best_epoch = 87 and best_val_0_unsup_loss_numpy = 0.7614399790763855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.7308  | train_logloss: 0.6739  | valid_logloss: 0.64184 |  0:00:01s\n",
      "epoch 1  | loss: 0.63446 | train_logloss: 0.63321 | valid_logloss: 0.60459 |  0:00:02s\n",
      "epoch 2  | loss: 0.6247  | train_logloss: 0.62651 | valid_logloss: 0.60049 |  0:00:03s\n",
      "epoch 3  | loss: 0.61764 | train_logloss: 0.6194  | valid_logloss: 0.59891 |  0:00:04s\n",
      "epoch 4  | loss: 0.61806 | train_logloss: 0.61714 | valid_logloss: 0.59522 |  0:00:05s\n",
      "epoch 5  | loss: 0.61509 | train_logloss: 0.61794 | valid_logloss: 0.59646 |  0:00:06s\n",
      "epoch 6  | loss: 0.61586 | train_logloss: 0.6158  | valid_logloss: 0.59511 |  0:00:07s\n",
      "epoch 7  | loss: 0.61695 | train_logloss: 0.61839 | valid_logloss: 0.59599 |  0:00:08s\n",
      "epoch 8  | loss: 0.61742 | train_logloss: 0.61597 | valid_logloss: 0.59829 |  0:00:09s\n",
      "epoch 9  | loss: 0.61764 | train_logloss: 0.6147  | valid_logloss: 0.59459 |  0:00:11s\n",
      "epoch 10 | loss: 0.61501 | train_logloss: 0.61584 | valid_logloss: 0.59514 |  0:00:12s\n",
      "epoch 11 | loss: 0.61613 | train_logloss: 0.61336 | valid_logloss: 0.59386 |  0:00:13s\n",
      "epoch 12 | loss: 0.61562 | train_logloss: 0.61306 | valid_logloss: 0.59597 |  0:00:14s\n",
      "epoch 13 | loss: 0.61768 | train_logloss: 0.61304 | valid_logloss: 0.59399 |  0:00:15s\n",
      "epoch 14 | loss: 0.6124  | train_logloss: 0.61596 | valid_logloss: 0.59636 |  0:00:16s\n",
      "epoch 15 | loss: 0.61501 | train_logloss: 0.61259 | valid_logloss: 0.59323 |  0:00:17s\n",
      "epoch 16 | loss: 0.61488 | train_logloss: 0.61439 | valid_logloss: 0.59532 |  0:00:18s\n",
      "epoch 17 | loss: 0.61551 | train_logloss: 0.61431 | valid_logloss: 0.59758 |  0:00:19s\n",
      "epoch 18 | loss: 0.61657 | train_logloss: 0.6125  | valid_logloss: 0.59389 |  0:00:20s\n",
      "epoch 19 | loss: 0.61618 | train_logloss: 0.61225 | valid_logloss: 0.59217 |  0:00:22s\n",
      "epoch 20 | loss: 0.61473 | train_logloss: 0.61445 | valid_logloss: 0.59256 |  0:00:23s\n",
      "epoch 21 | loss: 0.61565 | train_logloss: 0.61281 | valid_logloss: 0.59092 |  0:00:24s\n",
      "epoch 22 | loss: 0.61468 | train_logloss: 0.61322 | valid_logloss: 0.59146 |  0:00:25s\n",
      "epoch 23 | loss: 0.61553 | train_logloss: 0.6135  | valid_logloss: 0.59055 |  0:00:26s\n",
      "epoch 24 | loss: 0.61317 | train_logloss: 0.61302 | valid_logloss: 0.5901  |  0:00:27s\n",
      "epoch 25 | loss: 0.61271 | train_logloss: 0.61255 | valid_logloss: 0.58977 |  0:00:28s\n",
      "epoch 26 | loss: 0.61349 | train_logloss: 0.61107 | valid_logloss: 0.59089 |  0:00:29s\n",
      "epoch 27 | loss: 0.6125  | train_logloss: 0.61317 | valid_logloss: 0.59153 |  0:00:30s\n",
      "epoch 28 | loss: 0.61377 | train_logloss: 0.61241 | valid_logloss: 0.59019 |  0:00:31s\n",
      "epoch 29 | loss: 0.61721 | train_logloss: 0.61483 | valid_logloss: 0.59512 |  0:00:32s\n",
      "epoch 30 | loss: 0.61402 | train_logloss: 0.61232 | valid_logloss: 0.59037 |  0:00:34s\n",
      "epoch 31 | loss: 0.61199 | train_logloss: 0.6155  | valid_logloss: 0.59671 |  0:00:35s\n",
      "epoch 32 | loss: 0.61535 | train_logloss: 0.61292 | valid_logloss: 0.59056 |  0:00:36s\n",
      "epoch 33 | loss: 0.61892 | train_logloss: 0.61599 | valid_logloss: 0.59298 |  0:00:37s\n",
      "epoch 34 | loss: 0.61418 | train_logloss: 0.61198 | valid_logloss: 0.59224 |  0:00:38s\n",
      "epoch 35 | loss: 0.61383 | train_logloss: 0.61188 | valid_logloss: 0.59148 |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_valid_logloss = 0.58977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 35.71334| val_0_unsup_loss_numpy: 177.01922607421875|  0:00:00s\n",
      "epoch 1  | loss: 3.20178 | val_0_unsup_loss_numpy: 14.371740341186523|  0:00:01s\n",
      "epoch 2  | loss: 1.74427 | val_0_unsup_loss_numpy: 24.39349937438965|  0:00:02s\n",
      "epoch 3  | loss: 1.31387 | val_0_unsup_loss_numpy: 3.773940086364746|  0:00:03s\n",
      "epoch 4  | loss: 1.20053 | val_0_unsup_loss_numpy: 4.367929935455322|  0:00:04s\n",
      "epoch 5  | loss: 1.14815 | val_0_unsup_loss_numpy: 3.998610019683838|  0:00:05s\n",
      "epoch 6  | loss: 1.16136 | val_0_unsup_loss_numpy: 1.8066400289535522|  0:00:06s\n",
      "epoch 7  | loss: 1.08228 | val_0_unsup_loss_numpy: 1.2938300371170044|  0:00:07s\n",
      "epoch 8  | loss: 1.0377  | val_0_unsup_loss_numpy: 2.125309944152832|  0:00:08s\n",
      "epoch 9  | loss: 1.13309 | val_0_unsup_loss_numpy: 5.820350170135498|  0:00:08s\n",
      "epoch 10 | loss: 1.2986  | val_0_unsup_loss_numpy: 2.0673599243164062|  0:00:09s\n",
      "epoch 11 | loss: 1.02789 | val_0_unsup_loss_numpy: 0.9876000285148621|  0:00:10s\n",
      "epoch 12 | loss: 0.9812  | val_0_unsup_loss_numpy: 0.9557399749755859|  0:00:11s\n",
      "epoch 13 | loss: 1.05192 | val_0_unsup_loss_numpy: 1.2110199928283691|  0:00:12s\n",
      "epoch 14 | loss: 1.02211 | val_0_unsup_loss_numpy: 0.9465799927711487|  0:00:13s\n",
      "epoch 15 | loss: 1.04015 | val_0_unsup_loss_numpy: 3.8519299030303955|  0:00:14s\n",
      "epoch 16 | loss: 1.64017 | val_0_unsup_loss_numpy: 3.3571701049804688|  0:00:15s\n",
      "epoch 17 | loss: 1.16385 | val_0_unsup_loss_numpy: 3.088360071182251|  0:00:16s\n",
      "epoch 18 | loss: 1.08475 | val_0_unsup_loss_numpy: 0.9115599989891052|  0:00:16s\n",
      "epoch 19 | loss: 1.20373 | val_0_unsup_loss_numpy: 1.1366699934005737|  0:00:17s\n",
      "epoch 20 | loss: 1.24868 | val_0_unsup_loss_numpy: 5.424059867858887|  0:00:18s\n",
      "epoch 21 | loss: 1.25516 | val_0_unsup_loss_numpy: 2.6132400035858154|  0:00:19s\n",
      "epoch 22 | loss: 1.15882 | val_0_unsup_loss_numpy: 1.3247400522232056|  0:00:20s\n",
      "epoch 23 | loss: 1.17921 | val_0_unsup_loss_numpy: 4.100319862365723|  0:00:21s\n",
      "epoch 24 | loss: 1.14408 | val_0_unsup_loss_numpy: 0.9999300241470337|  0:00:22s\n",
      "epoch 25 | loss: 0.91445 | val_0_unsup_loss_numpy: 0.9085800051689148|  0:00:23s\n",
      "epoch 26 | loss: 0.90742 | val_0_unsup_loss_numpy: 0.8425700068473816|  0:00:24s\n",
      "epoch 27 | loss: 0.9148  | val_0_unsup_loss_numpy: 0.8499100208282471|  0:00:25s\n",
      "epoch 28 | loss: 0.90945 | val_0_unsup_loss_numpy: 0.8295400142669678|  0:00:25s\n",
      "epoch 29 | loss: 0.9339  | val_0_unsup_loss_numpy: 0.8973699808120728|  0:00:26s\n",
      "epoch 30 | loss: 0.89774 | val_0_unsup_loss_numpy: 0.9127500057220459|  0:00:27s\n",
      "epoch 31 | loss: 0.93408 | val_0_unsup_loss_numpy: 0.8510500192642212|  0:00:28s\n",
      "epoch 32 | loss: 0.95846 | val_0_unsup_loss_numpy: 1.7826999425888062|  0:00:29s\n",
      "epoch 33 | loss: 1.16106 | val_0_unsup_loss_numpy: 2.509229898452759|  0:00:30s\n",
      "epoch 34 | loss: 1.06818 | val_0_unsup_loss_numpy: 1.7612899541854858|  0:00:31s\n",
      "epoch 35 | loss: 0.94753 | val_0_unsup_loss_numpy: 0.8195199966430664|  0:00:32s\n",
      "epoch 36 | loss: 0.92937 | val_0_unsup_loss_numpy: 1.0915000438690186|  0:00:33s\n",
      "epoch 37 | loss: 0.86976 | val_0_unsup_loss_numpy: 1.556380033493042|  0:00:34s\n",
      "epoch 38 | loss: 1.0933  | val_0_unsup_loss_numpy: 0.9264100193977356|  0:00:34s\n",
      "epoch 39 | loss: 1.01097 | val_0_unsup_loss_numpy: 1.1799999475479126|  0:00:35s\n",
      "epoch 40 | loss: 0.89686 | val_0_unsup_loss_numpy: 0.9404100179672241|  0:00:36s\n",
      "epoch 41 | loss: 0.85936 | val_0_unsup_loss_numpy: 0.9340599775314331|  0:00:37s\n",
      "epoch 42 | loss: 0.87853 | val_0_unsup_loss_numpy: 0.8806599974632263|  0:00:38s\n",
      "epoch 43 | loss: 0.85694 | val_0_unsup_loss_numpy: 0.9712499976158142|  0:00:39s\n",
      "epoch 44 | loss: 0.88807 | val_0_unsup_loss_numpy: 0.9948099851608276|  0:00:40s\n",
      "epoch 45 | loss: 0.8966  | val_0_unsup_loss_numpy: 0.8736900091171265|  0:00:41s\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 35 and best_val_0_unsup_loss_numpy = 0.8195199966430664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.72688 | train_logloss: 0.69214 | valid_logloss: 0.69    |  0:00:01s\n",
      "epoch 1  | loss: 0.62343 | train_logloss: 0.65199 | valid_logloss: 0.65389 |  0:00:02s\n",
      "epoch 2  | loss: 0.61114 | train_logloss: 0.64245 | valid_logloss: 0.64615 |  0:00:03s\n",
      "epoch 3  | loss: 0.61264 | train_logloss: 0.62725 | valid_logloss: 0.63039 |  0:00:04s\n",
      "epoch 4  | loss: 0.61227 | train_logloss: 0.62245 | valid_logloss: 0.62698 |  0:00:05s\n",
      "epoch 5  | loss: 0.61029 | train_logloss: 0.61817 | valid_logloss: 0.62253 |  0:00:06s\n",
      "epoch 6  | loss: 0.60948 | train_logloss: 0.61227 | valid_logloss: 0.61977 |  0:00:07s\n",
      "epoch 7  | loss: 0.61072 | train_logloss: 0.61194 | valid_logloss: 0.61828 |  0:00:08s\n",
      "epoch 8  | loss: 0.612   | train_logloss: 0.60993 | valid_logloss: 0.61871 |  0:00:09s\n",
      "epoch 9  | loss: 0.61341 | train_logloss: 0.60933 | valid_logloss: 0.61686 |  0:00:11s\n",
      "epoch 10 | loss: 0.61248 | train_logloss: 0.60933 | valid_logloss: 0.61491 |  0:00:12s\n",
      "epoch 11 | loss: 0.61175 | train_logloss: 0.60948 | valid_logloss: 0.61744 |  0:00:13s\n",
      "epoch 12 | loss: 0.61142 | train_logloss: 0.60961 | valid_logloss: 0.61584 |  0:00:14s\n",
      "epoch 13 | loss: 0.60955 | train_logloss: 0.6092  | valid_logloss: 0.61485 |  0:00:15s\n",
      "epoch 14 | loss: 0.60771 | train_logloss: 0.60892 | valid_logloss: 0.61832 |  0:00:16s\n",
      "epoch 15 | loss: 0.60699 | train_logloss: 0.60784 | valid_logloss: 0.6146  |  0:00:17s\n",
      "epoch 16 | loss: 0.6084  | train_logloss: 0.60844 | valid_logloss: 0.61484 |  0:00:18s\n",
      "epoch 17 | loss: 0.60886 | train_logloss: 0.61194 | valid_logloss: 0.61845 |  0:00:19s\n",
      "epoch 18 | loss: 0.61017 | train_logloss: 0.60966 | valid_logloss: 0.61607 |  0:00:21s\n",
      "epoch 19 | loss: 0.61048 | train_logloss: 0.60786 | valid_logloss: 0.61606 |  0:00:22s\n",
      "epoch 20 | loss: 0.61027 | train_logloss: 0.6077  | valid_logloss: 0.61676 |  0:00:23s\n",
      "epoch 21 | loss: 0.60844 | train_logloss: 0.60746 | valid_logloss: 0.61536 |  0:00:24s\n",
      "epoch 22 | loss: 0.60673 | train_logloss: 0.60714 | valid_logloss: 0.61452 |  0:00:25s\n",
      "epoch 23 | loss: 0.61262 | train_logloss: 0.60746 | valid_logloss: 0.61465 |  0:00:26s\n",
      "epoch 24 | loss: 0.6084  | train_logloss: 0.60737 | valid_logloss: 0.6149  |  0:00:27s\n",
      "epoch 25 | loss: 0.60916 | train_logloss: 0.60694 | valid_logloss: 0.61674 |  0:00:28s\n",
      "epoch 26 | loss: 0.60957 | train_logloss: 0.60693 | valid_logloss: 0.61612 |  0:00:29s\n",
      "epoch 27 | loss: 0.61006 | train_logloss: 0.60821 | valid_logloss: 0.61689 |  0:00:31s\n",
      "epoch 28 | loss: 0.60748 | train_logloss: 0.60626 | valid_logloss: 0.61538 |  0:00:32s\n",
      "epoch 29 | loss: 0.6057  | train_logloss: 0.60756 | valid_logloss: 0.61668 |  0:00:33s\n",
      "epoch 30 | loss: 0.60806 | train_logloss: 0.60531 | valid_logloss: 0.61519 |  0:00:34s\n",
      "epoch 31 | loss: 0.60648 | train_logloss: 0.60575 | valid_logloss: 0.61547 |  0:00:35s\n",
      "epoch 32 | loss: 0.60749 | train_logloss: 0.60626 | valid_logloss: 0.61622 |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_valid_logloss = 0.61452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 28.38081| val_0_unsup_loss_numpy: 116.68115234375|  0:00:00s\n",
      "epoch 1  | loss: 3.30085 | val_0_unsup_loss_numpy: 6.321189880371094|  0:00:01s\n",
      "epoch 2  | loss: 1.67137 | val_0_unsup_loss_numpy: 3.409219980239868|  0:00:02s\n",
      "epoch 3  | loss: 1.36336 | val_0_unsup_loss_numpy: 2.843679904937744|  0:00:03s\n",
      "epoch 4  | loss: 1.2589  | val_0_unsup_loss_numpy: 2.1197500228881836|  0:00:04s\n",
      "epoch 5  | loss: 1.27764 | val_0_unsup_loss_numpy: 1.4433799982070923|  0:00:05s\n",
      "epoch 6  | loss: 1.12332 | val_0_unsup_loss_numpy: 2.4402499198913574|  0:00:06s\n",
      "epoch 7  | loss: 1.06985 | val_0_unsup_loss_numpy: 1.2499099969863892|  0:00:07s\n",
      "epoch 8  | loss: 1.15865 | val_0_unsup_loss_numpy: 2.115540027618408|  0:00:08s\n",
      "epoch 9  | loss: 1.01649 | val_0_unsup_loss_numpy: 1.3405499458312988|  0:00:08s\n",
      "epoch 10 | loss: 1.03331 | val_0_unsup_loss_numpy: 1.1784499883651733|  0:00:09s\n",
      "epoch 11 | loss: 1.02818 | val_0_unsup_loss_numpy: 1.2469899654388428|  0:00:10s\n",
      "epoch 12 | loss: 0.98553 | val_0_unsup_loss_numpy: 1.1309399604797363|  0:00:11s\n",
      "epoch 13 | loss: 0.997   | val_0_unsup_loss_numpy: 1.1097099781036377|  0:00:12s\n",
      "epoch 14 | loss: 1.10939 | val_0_unsup_loss_numpy: 1.9442600011825562|  0:00:13s\n",
      "epoch 15 | loss: 1.08566 | val_0_unsup_loss_numpy: 1.0707999467849731|  0:00:14s\n",
      "epoch 16 | loss: 1.02759 | val_0_unsup_loss_numpy: 1.0522700548171997|  0:00:15s\n",
      "epoch 17 | loss: 1.06857 | val_0_unsup_loss_numpy: 1.5229599475860596|  0:00:16s\n",
      "epoch 18 | loss: 1.10141 | val_0_unsup_loss_numpy: 1.0038199424743652|  0:00:17s\n",
      "epoch 19 | loss: 1.03256 | val_0_unsup_loss_numpy: 1.0587400197982788|  0:00:17s\n",
      "epoch 20 | loss: 0.95527 | val_0_unsup_loss_numpy: 1.0018099546432495|  0:00:18s\n",
      "epoch 21 | loss: 0.94489 | val_0_unsup_loss_numpy: 0.9733999967575073|  0:00:19s\n",
      "epoch 22 | loss: 0.94239 | val_0_unsup_loss_numpy: 0.9752399921417236|  0:00:20s\n",
      "epoch 23 | loss: 1.01327 | val_0_unsup_loss_numpy: 1.3490699529647827|  0:00:21s\n",
      "epoch 24 | loss: 0.92553 | val_0_unsup_loss_numpy: 0.9426299929618835|  0:00:22s\n",
      "epoch 25 | loss: 0.97924 | val_0_unsup_loss_numpy: 1.0355499982833862|  0:00:23s\n",
      "epoch 26 | loss: 1.15791 | val_0_unsup_loss_numpy: 4.235939979553223|  0:00:24s\n",
      "epoch 27 | loss: 1.46293 | val_0_unsup_loss_numpy: 8.534950256347656|  0:00:25s\n",
      "epoch 28 | loss: 1.07372 | val_0_unsup_loss_numpy: 0.9175300002098083|  0:00:26s\n",
      "epoch 29 | loss: 0.8984  | val_0_unsup_loss_numpy: 0.9472900032997131|  0:00:26s\n",
      "epoch 30 | loss: 0.93411 | val_0_unsup_loss_numpy: 1.0621700286865234|  0:00:27s\n",
      "epoch 31 | loss: 0.90619 | val_0_unsup_loss_numpy: 1.0630500316619873|  0:00:28s\n",
      "epoch 32 | loss: 0.951   | val_0_unsup_loss_numpy: 1.0066499710083008|  0:00:29s\n",
      "epoch 33 | loss: 0.87635 | val_0_unsup_loss_numpy: 1.1710699796676636|  0:00:30s\n",
      "epoch 34 | loss: 0.89136 | val_0_unsup_loss_numpy: 1.0216100215911865|  0:00:31s\n",
      "epoch 35 | loss: 0.88584 | val_0_unsup_loss_numpy: 1.118649959564209|  0:00:32s\n",
      "epoch 36 | loss: 0.98689 | val_0_unsup_loss_numpy: 0.8697599768638611|  0:00:33s\n",
      "epoch 37 | loss: 1.06484 | val_0_unsup_loss_numpy: 1.0492899417877197|  0:00:34s\n",
      "epoch 38 | loss: 0.9311  | val_0_unsup_loss_numpy: 0.8661699891090393|  0:00:34s\n",
      "epoch 39 | loss: 0.87863 | val_0_unsup_loss_numpy: 0.8815500140190125|  0:00:35s\n",
      "epoch 40 | loss: 0.86834 | val_0_unsup_loss_numpy: 0.8305500149726868|  0:00:36s\n",
      "epoch 41 | loss: 0.85252 | val_0_unsup_loss_numpy: 0.8442599773406982|  0:00:37s\n",
      "epoch 42 | loss: 0.86997 | val_0_unsup_loss_numpy: 0.8288999795913696|  0:00:38s\n",
      "epoch 43 | loss: 0.8454  | val_0_unsup_loss_numpy: 0.8424299955368042|  0:00:39s\n",
      "epoch 44 | loss: 0.84425 | val_0_unsup_loss_numpy: 0.8541600108146667|  0:00:40s\n",
      "epoch 45 | loss: 0.84753 | val_0_unsup_loss_numpy: 0.8318300247192383|  0:00:41s\n",
      "epoch 46 | loss: 0.92042 | val_0_unsup_loss_numpy: 1.3543399572372437|  0:00:42s\n",
      "epoch 47 | loss: 0.93018 | val_0_unsup_loss_numpy: 0.8752300143241882|  0:00:43s\n",
      "epoch 48 | loss: 0.98117 | val_0_unsup_loss_numpy: 0.8346099853515625|  0:00:44s\n",
      "epoch 49 | loss: 0.83872 | val_0_unsup_loss_numpy: 0.8345100283622742|  0:00:44s\n",
      "epoch 50 | loss: 0.83516 | val_0_unsup_loss_numpy: 0.8390499949455261|  0:00:45s\n",
      "epoch 51 | loss: 0.86023 | val_0_unsup_loss_numpy: 0.8630200028419495|  0:00:46s\n",
      "epoch 52 | loss: 0.84563 | val_0_unsup_loss_numpy: 1.0572400093078613|  0:00:47s\n",
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 42 and best_val_0_unsup_loss_numpy = 0.8288999795913696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [2, 4, 4, 2, 3, 5, 45, 59, 26, 65, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] to []\n",
      "  warnings.warn(wrn_msg)\n",
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73888 | train_logloss: 0.66243 | valid_logloss: 0.68047 |  0:00:01s\n",
      "epoch 1  | loss: 0.62156 | train_logloss: 0.62205 | valid_logloss: 0.64172 |  0:00:02s\n",
      "epoch 2  | loss: 0.61597 | train_logloss: 0.61653 | valid_logloss: 0.63132 |  0:00:03s\n",
      "epoch 3  | loss: 0.61316 | train_logloss: 0.61374 | valid_logloss: 0.6285  |  0:00:04s\n",
      "epoch 4  | loss: 0.61226 | train_logloss: 0.61406 | valid_logloss: 0.63053 |  0:00:05s\n",
      "epoch 5  | loss: 0.612   | train_logloss: 0.612   | valid_logloss: 0.6232  |  0:00:06s\n",
      "epoch 6  | loss: 0.61168 | train_logloss: 0.61004 | valid_logloss: 0.628   |  0:00:07s\n",
      "epoch 7  | loss: 0.6106  | train_logloss: 0.60701 | valid_logloss: 0.62141 |  0:00:08s\n",
      "epoch 8  | loss: 0.60685 | train_logloss: 0.60753 | valid_logloss: 0.62426 |  0:00:09s\n",
      "epoch 9  | loss: 0.60839 | train_logloss: 0.60518 | valid_logloss: 0.62114 |  0:00:11s\n",
      "epoch 10 | loss: 0.60828 | train_logloss: 0.60609 | valid_logloss: 0.62484 |  0:00:12s\n",
      "epoch 11 | loss: 0.60835 | train_logloss: 0.60554 | valid_logloss: 0.62472 |  0:00:13s\n",
      "epoch 12 | loss: 0.61044 | train_logloss: 0.60473 | valid_logloss: 0.62237 |  0:00:14s\n",
      "epoch 13 | loss: 0.60963 | train_logloss: 0.60676 | valid_logloss: 0.62527 |  0:00:15s\n",
      "epoch 14 | loss: 0.61083 | train_logloss: 0.60424 | valid_logloss: 0.62154 |  0:00:16s\n",
      "epoch 15 | loss: 0.60891 | train_logloss: 0.60523 | valid_logloss: 0.62515 |  0:00:17s\n",
      "epoch 16 | loss: 0.60806 | train_logloss: 0.60464 | valid_logloss: 0.61976 |  0:00:18s\n",
      "epoch 17 | loss: 0.6068  | train_logloss: 0.60506 | valid_logloss: 0.6216  |  0:00:19s\n",
      "epoch 18 | loss: 0.6067  | train_logloss: 0.60612 | valid_logloss: 0.62097 |  0:00:21s\n",
      "epoch 19 | loss: 0.6075  | train_logloss: 0.60461 | valid_logloss: 0.62205 |  0:00:22s\n",
      "epoch 20 | loss: 0.60542 | train_logloss: 0.60623 | valid_logloss: 0.62345 |  0:00:23s\n",
      "epoch 21 | loss: 0.60946 | train_logloss: 0.60426 | valid_logloss: 0.61972 |  0:00:24s\n",
      "epoch 22 | loss: 0.60481 | train_logloss: 0.60447 | valid_logloss: 0.61893 |  0:00:25s\n",
      "epoch 23 | loss: 0.60443 | train_logloss: 0.60447 | valid_logloss: 0.62119 |  0:00:26s\n",
      "epoch 24 | loss: 0.60551 | train_logloss: 0.60443 | valid_logloss: 0.62243 |  0:00:27s\n",
      "epoch 25 | loss: 0.60618 | train_logloss: 0.60376 | valid_logloss: 0.61977 |  0:00:28s\n",
      "epoch 26 | loss: 0.60582 | train_logloss: 0.6046  | valid_logloss: 0.62433 |  0:00:29s\n",
      "epoch 27 | loss: 0.6065  | train_logloss: 0.60317 | valid_logloss: 0.62124 |  0:00:30s\n",
      "epoch 28 | loss: 0.60319 | train_logloss: 0.60279 | valid_logloss: 0.62261 |  0:00:32s\n",
      "epoch 29 | loss: 0.60672 | train_logloss: 0.60416 | valid_logloss: 0.62338 |  0:00:33s\n",
      "epoch 30 | loss: 0.60818 | train_logloss: 0.60387 | valid_logloss: 0.62283 |  0:00:34s\n",
      "epoch 31 | loss: 0.60557 | train_logloss: 0.60219 | valid_logloss: 0.62086 |  0:00:35s\n",
      "epoch 32 | loss: 0.60426 | train_logloss: 0.60272 | valid_logloss: 0.62316 |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_valid_logloss = 0.61893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "col_target = \"health\"\n",
    "\n",
    "# train tabnet\n",
    "clf_lst, oof_pred = train_tabnet(train, cols_exp, cols_cat, col_target)\n",
    "\n",
    "# normalization for test\n",
    "x_train = train[cols_exp].to_numpy()\n",
    "x_test = test[cols_exp].to_numpy()\n",
    "cols_num_idxs = [i for i, c in enumerate(cols_exp) if not c in cols_cat]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train[:, cols_num_idxs])\n",
    "x_test[:, cols_num_idxs] = scaler.transform(x_test[:, cols_num_idxs])\n",
    "\n",
    "# predict test with CV ensemble\n",
    "y_test_pred = predict_test(x_test, clf_lst)\n",
    "\n",
    "# record\n",
    "oof_pred_df = pl.DataFrame(oof_pred, schema=[f\"health_is_{h}\" for h in range(3)])\n",
    "test_pred_df = pl.DataFrame(y_test_pred, schema=[f\"health_is_{h}\" for h in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "oof_pred_df.write_csv(f\"pred/oof_pred_tabnet_{feat}.csv\")\n",
    "test_pred_df.write_csv(f\"pred/test_pred_tabnet_{feat}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d00ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3ba5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
