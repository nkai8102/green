{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2ef3d0-c8bc-4511-934e-7205c5825dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca09f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"feat03\"\n",
    "train = pl.read_csv(f\"feat/feat_train_{feat}.csv\")\n",
    "test = pl.read_csv(f\"feat/feat_test_{feat}.csv\")\n",
    "train_origin = pl.read_csv(\"data/train.csv\").rename({\"\": \"idx\"})\n",
    "\n",
    "cols_exp = [c for c in test.columns if c != \"idx\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990efad",
   "metadata": {},
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3c42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabnet(train, cols_exp, col_target, params=None):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "        \n",
    "    params_add = {\"device_name\": \"cpu\", \"seed\": 0}\n",
    "    params |= params_add\n",
    "\n",
    "    x = train[cols_exp].to_numpy()\n",
    "    y = train[col_target].to_numpy()\n",
    "    \n",
    "    # K-fold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    y_valid_pred_lst = []\n",
    "    idx_valid_lst = []\n",
    "    clf_lst = []\n",
    "\n",
    "    # cross validation\n",
    "    for fold, (idx_train, idx_valid) in enumerate(kf.split(x)):\n",
    "        print(\"fold\", fold)\n",
    "        x_train = x[idx_train, :]\n",
    "        x_valid = x[idx_valid, :]\n",
    "        y_train = y[idx_train]\n",
    "        y_valid = y[idx_valid]\n",
    "        \n",
    "        # normalization\n",
    "        scaler = StandardScaler()\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_valid = scaler.transform(x_valid)\n",
    "\n",
    "        # modeling\n",
    "        clf = TabNetClassifier(**params)\n",
    "        clf.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=[\"logloss\"]\n",
    "        )   \n",
    "\n",
    "        # oof\n",
    "        y_valid_pred = clf.predict_proba(x_valid)\n",
    "        y_valid_pred_lst.append(y_valid_pred)\n",
    "        idx_valid_lst.append(idx_valid)\n",
    "        clf_lst.append(clf)\n",
    "\n",
    "    idx_valid = np.hstack(idx_valid_lst)\n",
    "    y_valid_pred = np.vstack(y_valid_pred_lst)\n",
    "    oof_pred = y_valid_pred[np.argsort(idx_valid)]\n",
    "\n",
    "    return clf_lst, oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960ae40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(x_test, clf_lst):\n",
    "    y_test_pred_lst = []\n",
    "\n",
    "    for clf in clf_lst:\n",
    "        y_test_pred = clf.predict_proba(x_test)\n",
    "        y_test_pred_lst.append(y_test_pred)\n",
    "\n",
    "    y_test_pred = np.mean(y_test_pred_lst, axis=0)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4adc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.81184 | train_logloss: 0.66039 | valid_logloss: 0.66231 |  0:00:01s\n",
      "epoch 1  | loss: 0.64572 | train_logloss: 0.62549 | valid_logloss: 0.62412 |  0:00:02s\n",
      "epoch 2  | loss: 0.6218  | train_logloss: 0.61621 | valid_logloss: 0.62384 |  0:00:03s\n",
      "epoch 3  | loss: 0.62015 | train_logloss: 0.61344 | valid_logloss: 0.62138 |  0:00:05s\n",
      "epoch 4  | loss: 0.61501 | train_logloss: 0.61009 | valid_logloss: 0.61645 |  0:00:06s\n",
      "epoch 5  | loss: 0.61478 | train_logloss: 0.60977 | valid_logloss: 0.61783 |  0:00:07s\n",
      "epoch 6  | loss: 0.60968 | train_logloss: 0.6086  | valid_logloss: 0.61463 |  0:00:08s\n",
      "epoch 7  | loss: 0.6117  | train_logloss: 0.60705 | valid_logloss: 0.61662 |  0:00:10s\n",
      "epoch 8  | loss: 0.61151 | train_logloss: 0.60884 | valid_logloss: 0.61782 |  0:00:11s\n",
      "epoch 9  | loss: 0.6113  | train_logloss: 0.60814 | valid_logloss: 0.61307 |  0:00:12s\n",
      "epoch 10 | loss: 0.61086 | train_logloss: 0.60836 | valid_logloss: 0.61404 |  0:00:13s\n",
      "epoch 11 | loss: 0.61184 | train_logloss: 0.60764 | valid_logloss: 0.61591 |  0:00:15s\n",
      "epoch 12 | loss: 0.60896 | train_logloss: 0.60747 | valid_logloss: 0.61448 |  0:00:16s\n",
      "epoch 13 | loss: 0.60804 | train_logloss: 0.60682 | valid_logloss: 0.61515 |  0:00:17s\n",
      "epoch 14 | loss: 0.60929 | train_logloss: 0.60684 | valid_logloss: 0.61581 |  0:00:19s\n",
      "epoch 15 | loss: 0.60799 | train_logloss: 0.60567 | valid_logloss: 0.61737 |  0:00:20s\n",
      "epoch 16 | loss: 0.60803 | train_logloss: 0.60541 | valid_logloss: 0.61495 |  0:00:21s\n",
      "epoch 17 | loss: 0.60786 | train_logloss: 0.60635 | valid_logloss: 0.6157  |  0:00:22s\n",
      "epoch 18 | loss: 0.60781 | train_logloss: 0.6053  | valid_logloss: 0.6123  |  0:00:24s\n",
      "epoch 19 | loss: 0.60702 | train_logloss: 0.60615 | valid_logloss: 0.61863 |  0:00:25s\n",
      "epoch 20 | loss: 0.60872 | train_logloss: 0.60935 | valid_logloss: 0.62017 |  0:00:26s\n",
      "epoch 21 | loss: 0.60833 | train_logloss: 0.60866 | valid_logloss: 0.61681 |  0:00:27s\n",
      "epoch 22 | loss: 0.60661 | train_logloss: 0.60693 | valid_logloss: 0.61756 |  0:00:29s\n",
      "epoch 23 | loss: 0.60974 | train_logloss: 0.60543 | valid_logloss: 0.61358 |  0:00:30s\n",
      "epoch 24 | loss: 0.60826 | train_logloss: 0.60636 | valid_logloss: 0.61234 |  0:00:31s\n",
      "epoch 25 | loss: 0.60735 | train_logloss: 0.60482 | valid_logloss: 0.61432 |  0:00:33s\n",
      "epoch 26 | loss: 0.60891 | train_logloss: 0.60473 | valid_logloss: 0.61364 |  0:00:34s\n",
      "epoch 27 | loss: 0.6049  | train_logloss: 0.60421 | valid_logloss: 0.61217 |  0:00:35s\n",
      "epoch 28 | loss: 0.60764 | train_logloss: 0.60525 | valid_logloss: 0.61298 |  0:00:36s\n",
      "epoch 29 | loss: 0.60747 | train_logloss: 0.60636 | valid_logloss: 0.61229 |  0:00:38s\n",
      "epoch 30 | loss: 0.60827 | train_logloss: 0.6042  | valid_logloss: 0.61217 |  0:00:39s\n",
      "epoch 31 | loss: 0.6077  | train_logloss: 0.60391 | valid_logloss: 0.61194 |  0:00:40s\n",
      "epoch 32 | loss: 0.60741 | train_logloss: 0.60634 | valid_logloss: 0.61243 |  0:00:41s\n",
      "epoch 33 | loss: 0.6082  | train_logloss: 0.60762 | valid_logloss: 0.61418 |  0:00:43s\n",
      "epoch 34 | loss: 0.60618 | train_logloss: 0.60322 | valid_logloss: 0.61284 |  0:00:44s\n",
      "epoch 35 | loss: 0.60811 | train_logloss: 0.6051  | valid_logloss: 0.61461 |  0:00:45s\n",
      "epoch 36 | loss: 0.60569 | train_logloss: 0.60248 | valid_logloss: 0.61125 |  0:00:47s\n",
      "epoch 37 | loss: 0.60436 | train_logloss: 0.60185 | valid_logloss: 0.61295 |  0:00:48s\n",
      "epoch 38 | loss: 0.60346 | train_logloss: 0.60404 | valid_logloss: 0.61487 |  0:00:49s\n",
      "epoch 39 | loss: 0.60579 | train_logloss: 0.60374 | valid_logloss: 0.6131  |  0:00:50s\n",
      "epoch 40 | loss: 0.60591 | train_logloss: 0.60359 | valid_logloss: 0.61343 |  0:00:52s\n",
      "epoch 41 | loss: 0.60328 | train_logloss: 0.60328 | valid_logloss: 0.61608 |  0:00:53s\n",
      "epoch 42 | loss: 0.60321 | train_logloss: 0.60165 | valid_logloss: 0.61473 |  0:00:54s\n",
      "epoch 43 | loss: 0.60344 | train_logloss: 0.60226 | valid_logloss: 0.61749 |  0:00:55s\n",
      "epoch 44 | loss: 0.60487 | train_logloss: 0.60214 | valid_logloss: 0.61537 |  0:00:57s\n",
      "epoch 45 | loss: 0.60568 | train_logloss: 0.60248 | valid_logloss: 0.61772 |  0:00:58s\n",
      "epoch 46 | loss: 0.60659 | train_logloss: 0.60504 | valid_logloss: 0.61349 |  0:00:59s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_valid_logloss = 0.61125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.7947  | train_logloss: 0.66145 | valid_logloss: 0.66717 |  0:00:01s\n",
      "epoch 1  | loss: 0.64153 | train_logloss: 0.62822 | valid_logloss: 0.63213 |  0:00:02s\n",
      "epoch 2  | loss: 0.62133 | train_logloss: 0.61512 | valid_logloss: 0.62471 |  0:00:03s\n",
      "epoch 3  | loss: 0.61774 | train_logloss: 0.61117 | valid_logloss: 0.62156 |  0:00:05s\n",
      "epoch 4  | loss: 0.6157  | train_logloss: 0.60967 | valid_logloss: 0.61815 |  0:00:06s\n",
      "epoch 5  | loss: 0.61376 | train_logloss: 0.60844 | valid_logloss: 0.61878 |  0:00:07s\n",
      "epoch 6  | loss: 0.6123  | train_logloss: 0.60752 | valid_logloss: 0.61753 |  0:00:09s\n",
      "epoch 7  | loss: 0.61061 | train_logloss: 0.60751 | valid_logloss: 0.61598 |  0:00:10s\n",
      "epoch 8  | loss: 0.61211 | train_logloss: 0.60775 | valid_logloss: 0.6169  |  0:00:11s\n",
      "epoch 9  | loss: 0.60904 | train_logloss: 0.60682 | valid_logloss: 0.61714 |  0:00:13s\n",
      "epoch 10 | loss: 0.60927 | train_logloss: 0.60547 | valid_logloss: 0.6171  |  0:00:14s\n",
      "epoch 11 | loss: 0.60562 | train_logloss: 0.60477 | valid_logloss: 0.61552 |  0:00:15s\n",
      "epoch 12 | loss: 0.6078  | train_logloss: 0.60471 | valid_logloss: 0.61656 |  0:00:16s\n",
      "epoch 13 | loss: 0.60927 | train_logloss: 0.60632 | valid_logloss: 0.61849 |  0:00:18s\n",
      "epoch 14 | loss: 0.60696 | train_logloss: 0.6045  | valid_logloss: 0.61415 |  0:00:19s\n",
      "epoch 15 | loss: 0.60383 | train_logloss: 0.60465 | valid_logloss: 0.6167  |  0:00:20s\n",
      "epoch 16 | loss: 0.60452 | train_logloss: 0.60358 | valid_logloss: 0.6158  |  0:00:21s\n",
      "epoch 17 | loss: 0.60553 | train_logloss: 0.60351 | valid_logloss: 0.61432 |  0:00:23s\n",
      "epoch 18 | loss: 0.60706 | train_logloss: 0.6046  | valid_logloss: 0.61587 |  0:00:24s\n",
      "epoch 19 | loss: 0.60743 | train_logloss: 0.60524 | valid_logloss: 0.61818 |  0:00:25s\n",
      "epoch 20 | loss: 0.60623 | train_logloss: 0.60316 | valid_logloss: 0.61617 |  0:00:27s\n",
      "epoch 21 | loss: 0.6042  | train_logloss: 0.60264 | valid_logloss: 0.61515 |  0:00:28s\n",
      "epoch 22 | loss: 0.60874 | train_logloss: 0.60576 | valid_logloss: 0.61773 |  0:00:29s\n",
      "epoch 23 | loss: 0.60678 | train_logloss: 0.60441 | valid_logloss: 0.61717 |  0:00:30s\n",
      "epoch 24 | loss: 0.60682 | train_logloss: 0.6027  | valid_logloss: 0.61435 |  0:00:32s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_valid_logloss = 0.61415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.81861 | train_logloss: 0.66041 | valid_logloss: 0.66651 |  0:00:01s\n",
      "epoch 1  | loss: 0.64704 | train_logloss: 0.63803 | valid_logloss: 0.64444 |  0:00:02s\n",
      "epoch 2  | loss: 0.63148 | train_logloss: 0.62515 | valid_logloss: 0.61653 |  0:00:03s\n",
      "epoch 3  | loss: 0.62415 | train_logloss: 0.62087 | valid_logloss: 0.60562 |  0:00:05s\n",
      "epoch 4  | loss: 0.6195  | train_logloss: 0.6162  | valid_logloss: 0.60021 |  0:00:06s\n",
      "epoch 5  | loss: 0.62027 | train_logloss: 0.61491 | valid_logloss: 0.59122 |  0:00:07s\n",
      "epoch 6  | loss: 0.61502 | train_logloss: 0.61498 | valid_logloss: 0.5911  |  0:00:09s\n",
      "epoch 7  | loss: 0.61598 | train_logloss: 0.61461 | valid_logloss: 0.59223 |  0:00:10s\n",
      "epoch 8  | loss: 0.61596 | train_logloss: 0.61244 | valid_logloss: 0.59124 |  0:00:11s\n",
      "epoch 9  | loss: 0.6139  | train_logloss: 0.61275 | valid_logloss: 0.59166 |  0:00:12s\n",
      "epoch 10 | loss: 0.61495 | train_logloss: 0.61222 | valid_logloss: 0.59123 |  0:00:14s\n",
      "epoch 11 | loss: 0.61463 | train_logloss: 0.61058 | valid_logloss: 0.5902  |  0:00:15s\n",
      "epoch 12 | loss: 0.61064 | train_logloss: 0.60923 | valid_logloss: 0.5885  |  0:00:16s\n",
      "epoch 13 | loss: 0.60887 | train_logloss: 0.61402 | valid_logloss: 0.59133 |  0:00:17s\n",
      "epoch 14 | loss: 0.61114 | train_logloss: 0.60942 | valid_logloss: 0.59053 |  0:00:19s\n",
      "epoch 15 | loss: 0.61025 | train_logloss: 0.61037 | valid_logloss: 0.58925 |  0:00:20s\n",
      "epoch 16 | loss: 0.61418 | train_logloss: 0.60988 | valid_logloss: 0.58978 |  0:00:21s\n",
      "epoch 17 | loss: 0.61368 | train_logloss: 0.60895 | valid_logloss: 0.59181 |  0:00:23s\n",
      "epoch 18 | loss: 0.61018 | train_logloss: 0.60917 | valid_logloss: 0.5947  |  0:00:24s\n",
      "epoch 19 | loss: 0.61103 | train_logloss: 0.60879 | valid_logloss: 0.59061 |  0:00:25s\n",
      "epoch 20 | loss: 0.61059 | train_logloss: 0.61052 | valid_logloss: 0.59365 |  0:00:26s\n",
      "epoch 21 | loss: 0.61088 | train_logloss: 0.60805 | valid_logloss: 0.58865 |  0:00:28s\n",
      "epoch 22 | loss: 0.60795 | train_logloss: 0.60807 | valid_logloss: 0.58845 |  0:00:29s\n",
      "epoch 23 | loss: 0.61194 | train_logloss: 0.60792 | valid_logloss: 0.59101 |  0:00:30s\n",
      "epoch 24 | loss: 0.61065 | train_logloss: 0.61086 | valid_logloss: 0.59829 |  0:00:31s\n",
      "epoch 25 | loss: 0.60953 | train_logloss: 0.6076  | valid_logloss: 0.59274 |  0:00:33s\n",
      "epoch 26 | loss: 0.61042 | train_logloss: 0.60723 | valid_logloss: 0.59496 |  0:00:34s\n",
      "epoch 27 | loss: 0.60894 | train_logloss: 0.60675 | valid_logloss: 0.59187 |  0:00:35s\n",
      "epoch 28 | loss: 0.60868 | train_logloss: 0.60587 | valid_logloss: 0.59012 |  0:00:37s\n",
      "epoch 29 | loss: 0.60574 | train_logloss: 0.60632 | valid_logloss: 0.59008 |  0:00:38s\n",
      "epoch 30 | loss: 0.60767 | train_logloss: 0.60689 | valid_logloss: 0.59649 |  0:00:39s\n",
      "epoch 31 | loss: 0.61075 | train_logloss: 0.60698 | valid_logloss: 0.59677 |  0:00:40s\n",
      "epoch 32 | loss: 0.60953 | train_logloss: 0.60503 | valid_logloss: 0.59097 |  0:00:42s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_valid_logloss = 0.58845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.8045  | train_logloss: 0.65694 | valid_logloss: 0.68137 |  0:00:01s\n",
      "epoch 1  | loss: 0.64184 | train_logloss: 0.62712 | valid_logloss: 0.6394  |  0:00:02s\n",
      "epoch 2  | loss: 0.62054 | train_logloss: 0.61196 | valid_logloss: 0.61955 |  0:00:03s\n",
      "epoch 3  | loss: 0.61498 | train_logloss: 0.61043 | valid_logloss: 0.61909 |  0:00:05s\n",
      "epoch 4  | loss: 0.61383 | train_logloss: 0.60919 | valid_logloss: 0.6192  |  0:00:06s\n",
      "epoch 5  | loss: 0.61053 | train_logloss: 0.60761 | valid_logloss: 0.61824 |  0:00:07s\n",
      "epoch 6  | loss: 0.60788 | train_logloss: 0.60647 | valid_logloss: 0.61584 |  0:00:08s\n",
      "epoch 7  | loss: 0.60781 | train_logloss: 0.60706 | valid_logloss: 0.62026 |  0:00:10s\n",
      "epoch 8  | loss: 0.60982 | train_logloss: 0.60687 | valid_logloss: 0.61493 |  0:00:11s\n",
      "epoch 9  | loss: 0.61128 | train_logloss: 0.60696 | valid_logloss: 0.61281 |  0:00:12s\n",
      "epoch 10 | loss: 0.60969 | train_logloss: 0.60633 | valid_logloss: 0.6176  |  0:00:14s\n",
      "epoch 11 | loss: 0.60586 | train_logloss: 0.60552 | valid_logloss: 0.61531 |  0:00:15s\n",
      "epoch 12 | loss: 0.60365 | train_logloss: 0.60527 | valid_logloss: 0.61717 |  0:00:16s\n",
      "epoch 13 | loss: 0.60848 | train_logloss: 0.60579 | valid_logloss: 0.61994 |  0:00:17s\n",
      "epoch 14 | loss: 0.60614 | train_logloss: 0.60439 | valid_logloss: 0.61519 |  0:00:19s\n",
      "epoch 15 | loss: 0.60531 | train_logloss: 0.60602 | valid_logloss: 0.62233 |  0:00:20s\n",
      "epoch 16 | loss: 0.60746 | train_logloss: 0.60513 | valid_logloss: 0.62347 |  0:00:21s\n",
      "epoch 17 | loss: 0.60529 | train_logloss: 0.60449 | valid_logloss: 0.61888 |  0:00:23s\n",
      "epoch 18 | loss: 0.60674 | train_logloss: 0.60437 | valid_logloss: 0.61676 |  0:00:24s\n",
      "epoch 19 | loss: 0.60812 | train_logloss: 0.60425 | valid_logloss: 0.61561 |  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_valid_logloss = 0.61281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.8152  | train_logloss: 0.67626 | valid_logloss: 0.67423 |  0:00:01s\n",
      "epoch 1  | loss: 0.64326 | train_logloss: 0.62548 | valid_logloss: 0.63865 |  0:00:02s\n",
      "epoch 2  | loss: 0.62159 | train_logloss: 0.61459 | valid_logloss: 0.62492 |  0:00:03s\n",
      "epoch 3  | loss: 0.61521 | train_logloss: 0.61119 | valid_logloss: 0.6234  |  0:00:05s\n",
      "epoch 4  | loss: 0.61453 | train_logloss: 0.61076 | valid_logloss: 0.62538 |  0:00:06s\n",
      "epoch 5  | loss: 0.61265 | train_logloss: 0.60912 | valid_logloss: 0.62315 |  0:00:07s\n",
      "epoch 6  | loss: 0.61035 | train_logloss: 0.60847 | valid_logloss: 0.62224 |  0:00:09s\n",
      "epoch 7  | loss: 0.61114 | train_logloss: 0.608   | valid_logloss: 0.62081 |  0:00:10s\n",
      "epoch 8  | loss: 0.60923 | train_logloss: 0.60744 | valid_logloss: 0.61992 |  0:00:11s\n",
      "epoch 9  | loss: 0.60807 | train_logloss: 0.6064  | valid_logloss: 0.62032 |  0:00:12s\n",
      "epoch 10 | loss: 0.61035 | train_logloss: 0.60551 | valid_logloss: 0.62048 |  0:00:14s\n",
      "epoch 11 | loss: 0.60769 | train_logloss: 0.60509 | valid_logloss: 0.61973 |  0:00:15s\n",
      "epoch 12 | loss: 0.6062  | train_logloss: 0.60528 | valid_logloss: 0.62044 |  0:00:16s\n",
      "epoch 13 | loss: 0.60829 | train_logloss: 0.60593 | valid_logloss: 0.61975 |  0:00:17s\n",
      "epoch 14 | loss: 0.6059  | train_logloss: 0.60364 | valid_logloss: 0.61984 |  0:00:19s\n",
      "epoch 15 | loss: 0.60464 | train_logloss: 0.60376 | valid_logloss: 0.62024 |  0:00:20s\n",
      "epoch 16 | loss: 0.608   | train_logloss: 0.60283 | valid_logloss: 0.62053 |  0:00:21s\n",
      "epoch 17 | loss: 0.60491 | train_logloss: 0.60343 | valid_logloss: 0.62144 |  0:00:22s\n",
      "epoch 18 | loss: 0.60367 | train_logloss: 0.6023  | valid_logloss: 0.62211 |  0:00:24s\n",
      "epoch 19 | loss: 0.60548 | train_logloss: 0.60313 | valid_logloss: 0.62217 |  0:00:25s\n",
      "epoch 20 | loss: 0.60698 | train_logloss: 0.60314 | valid_logloss: 0.62079 |  0:00:26s\n",
      "epoch 21 | loss: 0.60475 | train_logloss: 0.60364 | valid_logloss: 0.62169 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_valid_logloss = 0.61973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\venv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "col_target = \"health\"\n",
    "\n",
    "# train tabnet\n",
    "clf_lst, oof_pred = train_tabnet(train, cols_exp, col_target)\n",
    "\n",
    "# normalization for test\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[cols_exp].to_numpy())\n",
    "x_test = scaler.transform(test[cols_exp].to_numpy())\n",
    "\n",
    "# predict test with CV ensemble\n",
    "y_test_pred = predict_test(x_test, clf_lst)\n",
    "\n",
    "# record\n",
    "oof_pred_df = pl.DataFrame(oof_pred, schema=[f\"health_is_{h}\" for h in range(3)])\n",
    "test_pred_df = pl.DataFrame(y_test_pred, schema=[f\"health_is_{h}\" for h in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "oof_pred_df.write_csv(f\"pred/oof_pred_tabnet_{feat}.csv\")\n",
    "test_pred_df.write_csv(f\"pred/test_pred_tabnet_{feat}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d00ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3ba5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
